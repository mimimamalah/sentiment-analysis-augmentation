{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0Iyg6btLW9M"
   },
   "source": [
    "#  Assignment 2 - Transfer Learning and Data Augmentation üí¨\n",
    "\n",
    "Welcome to the **second assignment** for the **CS-552: Modern NLP course**!\n",
    "\n",
    "> - üòÄ Name: **< First Last >**\n",
    "> - ‚úâÔ∏è Email: **< first.last >@epfl.ch**\n",
    "> - ü™™ SCIPER: **XXXXXX**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XjnQhbFIJUu"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid green;background-color:#e4fae4;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Assignment Description**\n",
    "- In the first part of this assignment, you will need to implement training (finetuning) and evaluation of a pre-trained language model ([RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)) on a **Sentiment Analysis (SA)** task, which aims to determine whether a product review's emotional tone is positive or negative.\n",
    "\n",
    "- For part-2, following the first finetuning task, you will need to identify the shortcuts (i.e. some salient or toxic features) that the model learnt for the specific task.\n",
    "\n",
    "- For part-3, you are supposed to annotate 80 randomly assigned new datapoints as ground-truth labels. Additionally, the cross annotation should be conducted by another one or two annotators, and you will learn about how to calculate the agreement statistics as a significant characteristic reflecting the quality of a collected dataset.\n",
    "\n",
    "- For part-4, since the human annotation is quite time- and effort-consuming, there are plenty of ways to get silver-labels from automatic labeling to augment the dataset scale, e.g., paraphrasing each text input in different words without changing its meaning. You will use a [T5](https://huggingface.co/docs/transformers/en/model_doc/t5) paraphrase model to expand the training data of sentiment analysis, and evaluate the improvement of data augmentation.\n",
    "\n",
    "For Parts 1 and Part 2, you will need to complete the code in the corresponding `.py` files (`sa.py` for Part 1, `shortcut.py` for Part 2). You will be provided with the function descriptions and detailed instructions about the code snippet you need to write.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "- **PART 1: Sentiment Analysis (33 pts)**\n",
    "    - 1.1 Dataset Processing (10 pts)\n",
    "    - 1.2 Model Training and Evaluation (18 pts)\n",
    "    - 1.3 Fine-Grained Validation (5 pts)\n",
    "- **PART 2: Identify Model Shortcuts (22 pts)**\n",
    "    - 2.1 N-gram Pattern Extraction (6 pts)\n",
    "    - 2.2 Distill Potentially Useful Patterns (8 pts)\n",
    "    - 2.3 Case Study (8 pts)\n",
    "- **PART 3: Annotate New Data (25 pts)**\n",
    "    - 3.1 Write an Annotation Guideline (5 pts)\n",
    "    - 3.2 Annotate Your Datapoints with Partner(s) (8 pts)\n",
    "    - 3.3 Agreement Measure (12 pts)\n",
    "- **PART 4: Data Augmentation (20 pts)**\n",
    "    - 4.1 Data Augmentation with Paraphrasing (15 pts)\n",
    "    - 4.2 Retrain RoBERTa Model with Data Augmentation (5 pts)\n",
    "    \n",
    "### Deliverables\n",
    "\n",
    "- ‚úÖ This jupyter notebook: `assignment2.ipynb`\n",
    "- ‚úÖ `sa.py` and `shortcut.py` file\n",
    "- ‚úÖ Checkpoints for RoBERTa models finetuned on original and augmented SA training data (Part 1 and Part 4), including:\n",
    "    - `models/lr1e-05-warmup0.3/`\n",
    "    - `models/lr2e-05-warmup0.3/`\n",
    "    - `models/augmented/lr1e-05-warmup0.3/`\n",
    "- ‚úÖ Model prediction results on each domain data (Part 1.3 Fine-Grained Validation): `predictions/`\n",
    "- ‚úÖ Cross-annotated new SA data (Part 3), including:\n",
    "    - `data/<your_assigned_dataset_id>-<your_sciper_number>.jsonl`\n",
    "    - `data/<your_assigned_dataset_id>-<your_partner_sciper_number>.jsonl`\n",
    "    - (for group of 3) `data/<your_assigned_dataset_id>-<your_second_partner_sciper_number>.jsonl`\n",
    "- ‚úÖ Paraphrase-augmented SA training data (Part 4), including:\n",
    "    - `data/augmented_train_sa.jsonl`\n",
    "- ‚úÖ `./tensorboard` directory with logs for all trained/finetuned models, including:\n",
    "    - `tensorboard/part1_lr1e-05/`\n",
    "    - `tensorboard/part1_lr2e-05/`\n",
    "    - `tensorboard/part4_lr1e-05/`\n",
    "\n",
    "### How to implement this assignment\n",
    "\n",
    "Please read carefully the following points. All the information on how to read, implement and submit your assignment is explained in details below:\n",
    "\n",
    "1. For this assignment, you will need to implement and fill in the missing code snippets for both the **Jupyter Notebook `assignment2.ipynb`** and the **`sa.py`**, **`shortcut.py`** python files.\n",
    "\n",
    "2. Along with above files, you need to additionally upload model files under the **`models/`** dir, regarding the following models:\n",
    "    - finetuned RoBERTa models on original SA training data (PART 1)  \n",
    "    - finetuned RoBERTa model on augmented SA training data (PART 4)\n",
    "\n",
    "3. You also need to upload model prediction results in Part 1.3 Fine-Grained Validation, saved in **`predictions/`**.\n",
    "\n",
    "4. You also need to upload new data files under the **`data/`** dir (along with our already provided data), including:\n",
    "    - new SA data with your and your partner's annotations (Part 3)\n",
    "    - paraphrase-augmented SA training data (Part 4)\n",
    "\n",
    "5. Finally, you will need to log your training using Tensorboard. Please follow the instructions in the `README.md` of the **``tensorboard/``** directory.\n",
    "\n",
    "**Note**: Large files such as model checkpoints and logs should be pushed to the repository with Git LFS. You may also find that training the models on a GPU can speed up the process, we recommend using Colab's free GPU service for this. A tutorial on how to use Git LFS and Colab can be found [here](https://github.com/epfl-nlp/cs-552-modern-nlp/blob/main/Exercises/tutorials.md).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iJ_9sA2KyP2"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Environment Setup**\n",
    "\n",
    "### **Option 1: creating your own environment**\n",
    "\n",
    "```\n",
    "conda create --name mnlp-a2 python=3.10\n",
    "conda activate mnlp-a2\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Note**: If some package versions in our suggested environment do not work, feel free to try other package versions suitable for your computer, but remember to update ``requirements.txt`` and explain the environment changes in your notebook (no penalty for this if necessary).\n",
    "\n",
    "### **Option 2: using Google Colab**\n",
    "If you are using Google Colab notebook for this assignment, you will need to run a few commands to set up our environment on Google Colab, as shown below:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell makes sure modules are auto-loaded when you change external python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfVHqiSvK1aB"
   },
   "outputs": [],
   "source": [
    "# If you are working in Colab, then consider mounting your assignment folder to your drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Direct to your assignment folder.\n",
    "%cd /content/drive/MyDrive/path-to-your-assignment-folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LcGSuvWtmPf"
   },
   "source": [
    "Install packages that are not included in the Colab base envrionemnt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFXMx5FXtZhQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # limiting to one GPU\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jsonlines\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# TODO: Enter your Sciper number\n",
    "SCIPER = '...'\n",
    "seed = int(SCIPER)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the availability of GPU (proceed only it returns True!)\n",
    "if torch.cuda.is_available():\n",
    "  print('Good to go!')\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHhgkhaH-IUl"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "    \n",
    "# PART 1: Sentiment Analysis (33 pts)\n",
    "\n",
    "In this part, we will finetune a pretrained language model (Roberta) on sentiment analysis(SA) task. \n",
    "\n",
    "> Specifically, we will focus on a binary sentiment classification task for multi-domain product reviews. It requires the model to **classify a given paragraph of review by its sentiment polarity (positive or negative)**. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD2YPuqeIYBN"
   },
   "source": [
    "### Load Training Dataset (`train_sa.jsonl`) \n",
    "\n",
    "**You can run the following cell to have the first glance at your data**. Each data sample is a python dictionary, which consists of following components:\n",
    "- input review (*'review'*): a natural language sentence or a paragraph commenting about a product.\n",
    "- domain (*'domain'*): describing the type of product being reviewed.\n",
    "- label of sentiment (*'label'*): indicating whether the review states positive or negative views about the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-ODgcNUqYtm"
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "data_train_path = os.path.join(data_dir, 'train_sa.jsonl')\n",
    "with jsonlines.open(data_train_path, \"r\") as reader:\n",
    "    for sid, sample in enumerate(reader.iter()):\n",
    "        if sid % 200 == 0:\n",
    "            print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvM8jd_3QObg"
   },
   "outputs": [],
   "source": [
    "# We use the following pretrained tokenizer and model\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCETOFT2dB4u"
   },
   "source": [
    "## üéØ Q1.1: **Dataset Processing (10 pts)**\n",
    "\n",
    "Our first step is to constructing a Pytorch Dataset for SA task. Specifically, we will need to implement **tokenization** and **padding** using a HuggingFace pre-trained tokenizer.\n",
    "\n",
    "**TODOüîª: Complete `SADataset` class following the instructions in `sa.py`, and test by running the following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5Sya9W5BTDl"
   },
   "outputs": [],
   "source": [
    "from sa import SADataset\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "dataset = SADataset(\"data/train_sa.jsonl\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testA2 import test_SADataset\n",
    "test_SADataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0weQpG6_3vO"
   },
   "source": [
    "## üéØ Q1.2: **Model Training and Evaluation (18 pts)**\n",
    "\n",
    "Next, we will implement the training and evaluation process to finetune the model. \n",
    "\n",
    "- For training: you will need to calculate the **loss** and update the model weights by using **Adam optimizer**. Additionally, we add a **learning rate schedular** to adopt an adaptive learning rate during the whole training process.\n",
    "\n",
    "- For evaluation: you will need to compute the **confusion matrix** and **F1 scores** to assess the model performance.\n",
    "\n",
    "**TODOüîª: Complete the `compute_metrics()`, `train()` and `evaluate()` functions following the instructions in the `sa.py` file, you can test compute_metrics() by running the following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6w7Leraw4tIY"
   },
   "outputs": [],
   "source": [
    "from sa import compute_metrics, train, evaluate\n",
    "\n",
    "from testA2 import test_compute_metrics\n",
    "test_compute_metrics(compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvCUS748_3vS"
   },
   "source": [
    "#### **Start Training and Validation!**\n",
    "\n",
    "TODOüîª: (1) [coding question] Train the model with the following two different learning rates (other hyperparameters should be kept consistent). \n",
    "\n",
    "> A. learning_rate = 1e-5\n",
    "\n",
    "> B. learning_rate = 2e-5\n",
    "\n",
    "**Note:** *Each training will take ~7-10 minutes using a T4 Colab GPU.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zn66mMOj_3vS"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "warmup_percent = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5aWqR1h_3vS"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-5  # play around with this hyperparameter\n",
    "\n",
    "train(...,\n",
    "      model_save_root='models/', tensorboard_path=\"./tensorboard/part1_lr{}\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOüîª: (2) [textual question] compare and discuss the results. \n",
    "\n",
    "- Which learning rate is better? Explain your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGuzGJCB_3vT"
   },
   "source": [
    "## üéØ Q1.3: **Fine-Grained Validation (5 pts)**\n",
    "\n",
    "TODOüîª: (1) [coding question] Use the model checkpoint trained from the first learning_rate setting (lr=1e-5), check the model performance on each domain subsets of the validation set. You should report **the validation loss**, **confusion matrix**, **F1 scores** and **Macro-F1 on each domain**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCWWJjTP_3vT"
   },
   "outputs": [],
   "source": [
    "# Split the test sets into subsets with different domains\n",
    "# Save the subsets under 'data/'\n",
    "# Replace \"...\" with your code\n",
    "domain_data = {}\n",
    "...\n",
    "\n",
    "for domain, samples in domain_data.items():\n",
    "    with jsonlines.open(\"data/test_sa_\"+domain+\".jsonl\", mode=\"w\") as writer:\n",
    "        for sd in samples:\n",
    "            writer.write(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4J2pu60xHTd"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = ...\n",
    "model = ...\n",
    "model.to(device)\n",
    "\n",
    "results_save_dir = 'predictions/'\n",
    "\n",
    "# Evaluate and save prediction results in each domain\n",
    "# Replace \"...\" with your code\n",
    "for domain in [...]:\n",
    "\n",
    "    dev_loss, confusion, f1_pos, f1_neg = evaluate(...,\n",
    "                                                   result_save_file='predictions/test_'+domain+'.jsonl')\n",
    "    macro_f1 = (f1_pos + f1_neg) / 2\n",
    "\n",
    "    print(f'Domain: {domain}')\n",
    "    print(f'Validation Loss: {dev_loss:.3f}')\n",
    "    print(f'Confusion Matrix:')\n",
    "    print(confusion)\n",
    "    print(f'F1: ({f1_pos*100:.2f}%, {f1_neg*100:.2f}%) | Macro-F1: {macro_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOüîª: (2) [textual question] compare and discuss the results. \n",
    "\n",
    "**Questions:**\n",
    "- On which domain does the model perform the best? the worst?\n",
    "- Give some possible explanations of why the model's best-performed domain is easier, and why the model's worst-performed domain is more challenging. Use some examples to support your explanations.\n",
    "\n",
    "**Note:** To find examples for supporting your discussion, save the model prediction results on each domain under the `predictions/` folder, by specifying the `result_save_file` parameter in the *evaluate* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8j7l7mrS2iH"
   },
   "source": [
    "(Write your answer to the questions here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NyMZ5E4-QxM"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "# PART 2: Identify Model Shortcuts (22 pts)\n",
    "\n",
    "In this part, We aim to find out the shortcut features learnt by the sentiment analysis model we have trained in Part1. We will be using the model checkpoint trained with `learning rate=1e-5`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lCHLdaH_3vT"
   },
   "source": [
    "## üéØ Q2.1: **N-gram Pattern Extraction (6 pts)**\n",
    "We hypothesize that `n-gram`s could be the potential shortcut features learnt by the SA model. An `n-gram` is defined as a sequence of n consecutive words appeared in a natural language sentence or paragraph. \n",
    "\n",
    "Thus, we aim to extract that an n-gram that appears in a review may serve as a key indicator of the polarity of the review's sentiment, for example:\n",
    "\n",
    ">- **Review 1**: This book was **horrible**. If it was possible to rate it **lower than one star** I would have.\n",
    ">- **Review 2**: **Excellent** book, **highly recommended**. Helps to put a realistic perspective on millionaires.\n",
    "\n",
    "For Review 1, the `1-gram \"horrible\"` and the `4-gram \"lower than one star\"` serve as two key indicators of negative sentiment. While for Review 2, the `1-gram \"excellent\"` and the `2-gram \"highly recommended\"` obviously indicate positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NovYRxv_3vU"
   },
   "source": [
    "TODOüîª: (1) [coding question] Complete `ngram_extraction()` function in `shortcut.py` file.\n",
    "\n",
    "The returned *ngrams* contains a **list** of dictionaries. The `n-th` **dictionary** corresponds the `n-grams` (n=1,2, 3, 4).\n",
    "\n",
    "The keys of each dictionary should be a **unique n-gram string** appeared in reviews, and the value of each n-gram key records the frequency of positive/negative predictions **made by the model** when the n-gram appears in the review, i.e., `\\[#positive_predictions, #negative_predictions\\]`.\n",
    "\n",
    "> Example: **`ngrams`[0]['horrible'][0]** should return the number of the positive predictions made by the model when the 1-gram token 'horrible' appear in the given review. i.e., \\[#positive_predictions, #negative_predictions\\].\n",
    "\n",
    "**Note:** (1) All the sequences contain punctuations should NOT be counted as a n-gram (e.g. `it is great .` is NOT a 4-gram, but `it is great` is a 3-gram); (2) All stop-words should NOT be counted as 1-grams, but can appear in other n-gram sequences (e.g. `is` is NOT a 1-gram token, but `it is great` can be a 3-gram token.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTHt1frZ_3vU"
   },
   "source": [
    "## üéØ Q2.2: **Distill Potentially Useful Patterns (8 pts)**\n",
    "\n",
    "TODOüîª: (2) [coding question] For each group of n-grams (n=1,2,3,4), find and **print** the **top-100 n-gram sequences** with the **greatest frequency of appearance**, which could contain frequent semantic features and would be used as our feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFZm6tFJFmlg"
   },
   "outputs": [],
   "source": [
    "from shortcut import ngram_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUyal5mW_3vU"
   },
   "outputs": [],
   "source": [
    "# all your saved model prediction results from 1.3 Fine-Grained Validation\n",
    "prediction_files = [...]\n",
    "\n",
    "# TODO: Define your tokenizer\n",
    "tokenizer = ...\n",
    "ngrams = ngram_extraction(prediction_files, tokenizer)\n",
    "\n",
    "top_100 = {}\n",
    "for n, counts in enumerate(ngrams):\n",
    "    # TODO: find top-100 n-grams (n=1,2,3 or 4) associated with the greatest frequency of appearance\n",
    "    top_100_freq = ...\n",
    "\n",
    "    print(f'Top-100 most frequent {n+1}-grams:')\n",
    "    print(top_100_freq)\n",
    "\n",
    "    top_100[n] = top_100_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz25EvuI_3vU"
   },
   "source": [
    "**Among each type of top-100 frequent n-grams above**, we aim to further find out the n-grams which **most likely** lead to *positive*/*negative* predictions (positive/negative shortcut features). \n",
    "\n",
    "TODOüîª: (3) [coding&text question] Design **two different methods to re-rank** the top-100 n-grams to extract shortcut features. For each method, you should extract **1** feature in each of n-grams group (n=1, 2, 3, 4) for positve and negative prediction (1\\*4\\*2=8 features in total for 1 method).\n",
    "\n",
    "Explain each of your design choices in natural language, and compare which method finds more reasonable patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yq2cVOaWTEYw"
   },
   "outputs": [],
   "source": [
    "# TODO: [Method 1] find top-1 positive and negative patterns\n",
    "...\n",
    "\n",
    "# TODO: [Explanation of Method 1]\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yq2cVOaWTEYw"
   },
   "outputs": [],
   "source": [
    "# TODO: [Method 2] find top-1 positive and negative patterns\n",
    "...\n",
    "\n",
    "# TODO: [Explanation of Method 2]\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvKyF0UFuXXM"
   },
   "source": [
    "TODOüîª: Compare and discuss the results from two methods above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjVti-vL_3vV"
   },
   "source": [
    "## üéØ Q2.3: **Case Study (8 pts)**\n",
    "\n",
    "TODOüîª: Among the shortcut features you found in 2.1, find out **4 representative** cases (pair of `\\[review, n-gram feature\\]`) where the shortcut feature **will lead to a wrong prediction**. \n",
    "\n",
    "For example, the 1-gram feature \"excellent\" has been considered as a shortcut for *positive* sentiment, while the ground-truth label of the given review containing \"excellent\" is *negative*.\n",
    "\n",
    "**Questions:**\n",
    "- Based on your case study, do you detect any limitations of the n-gram patterns?\n",
    "- Which type of n-gram (1/2/3/4-gram) pattern is more robust to be used for sentiment prediction shortcut and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XyPmb01_3vV"
   },
   "outputs": [],
   "source": [
    "# TODO: you can fill your code for finding cases here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c45cvlUqufRR"
   },
   "source": [
    "TODOüîª: (Write your case study discussions and answers to the questions here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yND0DEfT-eXn"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Part 3: Annotate New Data (25 pts)**\n",
    "\n",
    "In this part, you will **annotate** the gold labels of some **new** SA data samples, and measure the degree of **agreement** between your and **one or two partners'** annotations.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQNXrRHr_3vV"
   },
   "source": [
    "## üéØ Q3.1: **Write an Annotation Guideline (5 pts)**\n",
    "\n",
    "TODOüîª: Imagine that you are going to assign this annotation task to a crowdsourcing worker, who is completely not familiar with computer science and NLP. Think about how you are going to explain this annotation task to him in order to guide him do a decent job. Write an annotation guideline for such a worker who are going to do this task for you.\n",
    "\n",
    "**Note:** You should come up with your own guideline without the help of your partner(s) in later Part 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfkqNbUA_3vV"
   },
   "source": [
    "(Write your annotation guideline here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBWK4Bw__3vV"
   },
   "source": [
    "## üéØ Q3.2: **Annotate Your Datapoints with Partner(s) (8 pts)**\n",
    "\n",
    "TODOüîª: Annotate 80 datapoints (20 in each domain of \"books\", \"dvd\", \"electronics\" and \"housewares\") assigned to you and your partner(s), by editing the value of the key **\"label\"** in each datapoint. You and your partner(s) should annotate **independently of each other**, i.e., each of you provide your own 80 annotations.\n",
    "\n",
    "Please find your assigned annotation dataset **ID** and **your partner(s)** according to this [list](https://docs.google.com/spreadsheets/d/1hOwBUb8XE8fitYa4hlAwq8mARZe3ZsL4/edit?usp=sharing&ouid=108194779329215429936&rtpof=true&sd=true). Your annotation dataset can be found [here](https://drive.google.com/drive/folders/1IHXU_v3PDGbZG6r9T5LdjKJkHQ351Mb4?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWhjTn2fQ5YE"
   },
   "source": [
    "**Name your annotated file as `<your_assigned_dataset_id>-<your_sciper_number>.jsonl`.**\n",
    "\n",
    "**You should also submit your partner's annotated file `<assigned_dataset_id>-<your_partner_sciper_number>.jsonl`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyP0GtHe_3vW"
   },
   "source": [
    "## üéØ Q3.3: **Agreement Measure (12 pts)**\n",
    "\n",
    "TODOüîª: Based on your and your partner's annotations in 3.2, calculate the [Cohen's Kappa](https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-kappa) or [Krippendorff's Alpha](https://github.com/pln-fing-udelar/fast-krippendorff) (if you are in a group of three students) between the annotators on **each domain** and **across all domains**.\n",
    "\n",
    "**Note:** Cohen's Kappa or Krippendorff's Alpha interpretation\n",
    "\n",
    "0: No Agreement\n",
    "\n",
    "0 ~ 0.2: Slight Agreement\n",
    "\n",
    "0.2 ~ 0.4: Fair Agreement\n",
    "\n",
    "0.4 ~ 0.6: Moderate Agreement\n",
    "\n",
    "0.6 ~ 0.8: Substantial Agreement\n",
    "\n",
    "0.8 ~ 1.0: Near Perfect Agreement\n",
    "\n",
    "1.0: Perfect Agreement\n",
    "\n",
    "**Questions:**\n",
    "- What is the overall degree of agreement between you and your partner(s) according to the above interpretation of score ranges?\n",
    "- In which domain are disagreements most and least frequently happen between you and your partner(s)? Give some examples to explain why that is the case.\n",
    "- Are there possible ways to address the disagreements between annotators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_2VlClO38Jt"
   },
   "outputs": [],
   "source": [
    "# Fill your code for calculating agreement scores here.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I98NGWYb3zl8"
   },
   "source": [
    "(Write your answers to the questions here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4wuRpHt-rQF"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Part 4: Data Augmentation (20 pts)**\n",
    "\n",
    "Since we only used 20% of the whole dataset for training, which might limit the model performance. In the final part, we will try to enlarge the training set by **data augmentation**.  \n",
    "\n",
    "Specifically, we will **`Rephrase`** some current training samples using pretrained paraphraser. So that the paraphrased synthetic samples would preserve the semantic similarity while change the surface format.\n",
    "\n",
    "You can use the pretrained T5 paraphraser [here](https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQa2q1io_5Pk"
   },
   "source": [
    "## üéØ Q4.1: **Data Augmentation with Paraphrasing (15 pts)**\n",
    "TODOüîª: Implement functions named `get_paraphrase_batch` and `get_paraphrase_dataset` with the details in the below two blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTMdZ-azABk-"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# get the given pretrained paraphrase model and the corresponding tokenizer (https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base)\n",
    "paraphrase_tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
    "\n",
    "def get_paraphrase_batch(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_samples,\n",
    "    n,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=256,\n",
    "    device='cuda:0'):\n",
    "    '''\n",
    "    Input\n",
    "      model: paraphraser\n",
    "      tokenizer: paraphrase tokenizer\n",
    "      input_samples: a batch (list) of real samples to be paraphrased\n",
    "      n: number of paraphrases to get for each input sample\n",
    "      for other parameters, please refer to:\n",
    "          https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    Output: Tuple.\n",
    "      synthetic_samples: a list of paraphrased samples\n",
    "    '''\n",
    "\n",
    "    # TODO: implement paraphrasing on a batch of imput samples\n",
    "    ...\n",
    "\n",
    "    return synthetic_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aL29QCAodF1b"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_dir = 'data'\n",
    "data_train_path = os.path.join(data_dir, 'train_sa.jsonl')\n",
    "BATCH_SIZE = 8\n",
    "N_PARAPHRASE = 2\n",
    "\n",
    "def get_paraphrase_dataset(model, tokenizer, data_path, batch_size, n_paraphrase):\n",
    "    '''\n",
    "    Input\n",
    "      model: paraphrase model\n",
    "      tokenizer: paraphrase tokenizer\n",
    "      data_path: path to the `jsonl` file of training data\n",
    "      batch_size: number of input samples to be paraphrases in one batch\n",
    "      n_paraphrase: number of paraphrased sequences for each sample\n",
    "    Output:\n",
    "      paraphrase_dataset: a list of all paraphrase samples. Do not include the original training data.\n",
    "    '''\n",
    "    paraphrase_dataset = []\n",
    "    with jsonlines.open(data_path, \"r\") as reader:\n",
    "\n",
    "        # TODO: get paraphrases for the whole training dataset using get_paraphrase_batch\n",
    "        ...\n",
    "\n",
    "    return paraphrase_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** run paraphrasing, which will take ~20-30 minutes using a T4 Colab GPU. But the running time could depend on various implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase_dataset = get_paraphrase_dataset(paraphrase_model, paraphrase_tokenizer, data_train_path, BATCH_SIZE, N_PARAPHRASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGQsvD4dktVv"
   },
   "outputs": [],
   "source": [
    "# Original training dataset\n",
    "with jsonlines.open(data_train_path, \"r\") as reader:\n",
    "    origin_data = [dt for dt in reader.iter()]\n",
    "\n",
    "all_data = origin_data + paraphrase_dataset\n",
    "\n",
    "# Write all the original and paraphrased data samples into training dataset\n",
    "augmented_data_train_path = os.path.join(data_dir, 'augmented_train_sa.jsonl')\n",
    "with jsonlines.open(augmented_data_train_path, \"w\") as writer:\n",
    "    writer.write_all(all_data)\n",
    "\n",
    "assert len(all_data) == 3 * len(origin_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmFpfxrjWA1O"
   },
   "source": [
    "## üéØ Q4.2: **Retrain RoBERTa Model with Data Augmentation (5 pts)** \n",
    "TODOüîª: Retrain the sentiment analysis model with the augmented (original+paraphrased), larger dataset :)\n",
    "\n",
    "**Note:** *Training on the augmented data will take about 15 minutes using a T4 Colab GPU.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sz9gQSe8ANix"
   },
   "outputs": [],
   "source": [
    "# Re-train a RoBERTa SA model on the augmented training dataset\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "train(...,\n",
    "      model_save_root='models/augmented/', tensorboard_path=\"./tensorboard/part4_lr{}\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PRvIB7ZAyaE"
   },
   "source": [
    "TODOüîª: Discuss your results by answering the following questions\n",
    "\n",
    "- Compare the performances of models in Part 1 and Part 4. Does the data augmentation help with the performance and why (give possible reasons)?\n",
    "- No matter whether the data augmentation helps or not, list **three** possible ways to improve our current data augmentation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5-dVSsniH9N"
   },
   "source": [
    "(Write your answers to the questions here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3CIeN_kaOQl"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "### **5 Upload Your Notebook, Data and Models**\n",
    "\n",
    "Please upload your filled jupyter notebook in your GitHub Classroom repository, **with all cells run and output results shown**.\n",
    "\n",
    "**Note:** We are **not** responsible for re-running the cells in your notebook.\n",
    "\n",
    "Please also submit all your **datasets** **(anotated and augmented)**, as well as **all your trained models** in Part 1 and Part 4, in your GitHub Classroom repository.\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
