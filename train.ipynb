{"cells":[{"cell_type":"markdown","metadata":{"id":"P0Iyg6btLW9M"},"source":["#  Assignment 2 - Transfer Learning and Data Augmentation ðŸ’¬\n","\n","Welcome to the **second assignment** for the **CS-552: Modern NLP course**!\n","\n","> - ðŸ˜€ Name: Malak Lahlou Nabil\n","> - âœ‰ï¸ Email: malak.lahlounabil@epfl.ch\n","> - ðŸªª SCIPER: 329571"]},{"cell_type":"markdown","metadata":{"id":"_XjnQhbFIJUu"},"source":["<div style=\"padding:15px 20px 20px 20px;border-left:3px solid green;background-color:#e4fae4;border-radius: 20px;color:#424242;\">\n","\n","## **Assignment Description**\n","- In the first part of this assignment, you will need to implement training (finetuning) and evaluation of a pre-trained language model ([RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)) on a **Sentiment Analysis (SA)** task, which aims to determine whether a product review's emotional tone is positive or negative.\n","\n","- For part-2, following the first finetuning task, you will need to identify the shortcuts (i.e. some salient or toxic features) that the model learnt for the specific task.\n","\n","- For part-3, you are supposed to annotate 80 randomly assigned new datapoints as ground-truth labels. Additionally, the cross annotation should be conducted by another one or two annotators, and you will learn about how to calculate the agreement statistics as a significant characteristic reflecting the quality of a collected dataset.\n","\n","- For part-4, since the human annotation is quite time- and effort-consuming, there are plenty of ways to get silver-labels from automatic labeling to augment the dataset scale, e.g., paraphrasing each text input in different words without changing its meaning. You will use a [T5](https://huggingface.co/docs/transformers/en/model_doc/t5) paraphrase model to expand the training data of sentiment analysis, and evaluate the improvement of data augmentation.\n","\n","For Parts 1 and Part 2, you will need to complete the code in the corresponding `.py` files (`sa.py` for Part 1, `shortcut.py` for Part 2). You will be provided with the function descriptions and detailed instructions about the code snippet you need to write.\n","\n","\n","### Table of Contents\n","- **PART 1: Sentiment Analysis (33 pts)**\n","    - 1.1 Dataset Processing (10 pts)\n","    - 1.2 Model Training and Evaluation (18 pts)\n","    - 1.3 Fine-Grained Validation (5 pts)\n","- **PART 2: Identify Model Shortcuts (22 pts)**\n","    - 2.1 N-gram Pattern Extraction (6 pts)\n","    - 2.2 Distill Potentially Useful Patterns (8 pts)\n","    - 2.3 Case Study (8 pts)\n","- **PART 3: Annotate New Data (25 pts)**\n","    - 3.1 Write an Annotation Guideline (5 pts)\n","    - 3.2 Annotate Your Datapoints with Partner(s) (8 pts)\n","    - 3.3 Agreement Measure (12 pts)\n","- **PART 4: Data Augmentation (20 pts)**\n","    - 4.1 Data Augmentation with Paraphrasing (15 pts)\n","    - 4.2 Retrain RoBERTa Model with Data Augmentation (5 pts)\n","    \n","### Deliverables\n","\n","- âœ… This jupyter notebook: `assignment2.ipynb`\n","- âœ… `sa.py` and `shortcut.py` file\n","- âœ… Checkpoints for RoBERTa models finetuned on original and augmented SA training data (Part 1 and Part 4), including:\n","    - `models/lr1e-05-warmup0.3/`\n","    - `models/lr2e-05-warmup0.3/`\n","    - `models/augmented/lr1e-05-warmup0.3/`\n","- âœ… Model prediction results on each domain data (Part 1.3 Fine-Grained Validation): `predictions/`\n","- âœ… Cross-annotated new SA data (Part 3), including:\n","    - `data/<your_assigned_dataset_id>-<your_sciper_number>.jsonl`\n","    - `data/<your_assigned_dataset_id>-<your_partner_sciper_number>.jsonl`\n","    - (for group of 3) `data/<your_assigned_dataset_id>-<your_second_partner_sciper_number>.jsonl`\n","- âœ… Paraphrase-augmented SA training data (Part 4), including:\n","    - `data/augmented_train_sa.jsonl`\n","- âœ… `./tensorboard` directory with logs for all trained/finetuned models, including:\n","    - `tensorboard/part1_lr1e-05/`\n","    - `tensorboard/part1_lr2e-05/`\n","    - `tensorboard/part4_lr1e-05/`\n","\n","### How to implement this assignment\n","\n","Please read carefully the following points. All the information on how to read, implement and submit your assignment is explained in details below:\n","\n","1. For this assignment, you will need to implement and fill in the missing code snippets for both the **Jupyter Notebook `assignment2.ipynb`** and the **`sa.py`**, **`shortcut.py`** python files.\n","\n","2. Along with above files, you need to additionally upload model files under the **`models/`** dir, regarding the following models:\n","    - finetuned RoBERTa models on original SA training data (PART 1)  \n","    - finetuned RoBERTa model on augmented SA training data (PART 4)\n","\n","3. You also need to upload model prediction results in Part 1.3 Fine-Grained Validation, saved in **`predictions/`**.\n","\n","4. You also need to upload new data files under the **`data/`** dir (along with our already provided data), including:\n","    - new SA data with your and your partner's annotations (Part 3)\n","    - paraphrase-augmented SA training data (Part 4)\n","\n","5. Finally, you will need to log your training using Tensorboard. Please follow the instructions in the `README.md` of the **``tensorboard/``** directory.\n","\n","**Note**: Large files such as model checkpoints and logs should be pushed to the repository with Git LFS. You may also find that training the models on a GPU can speed up the process, we recommend using Colab's free GPU service for this. A tutorial on how to use Git LFS and Colab can be found [here](https://github.com/epfl-nlp/cs-552-modern-nlp/blob/main/Exercises/tutorials.md).\n","    \n","</div>"]},{"cell_type":"markdown","metadata":{"id":"9iJ_9sA2KyP2"},"source":["<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n","\n","## **Environment Setup**\n","\n","### **Option 1: creating your own environment**\n","\n","```\n","conda create --name mnlp-a2 python=3.10\n","conda activate mnlp-a2\n","pip install -r requirements.txt\n","```\n","\n","**Note**: If some package versions in our suggested environment do not work, feel free to try other package versions suitable for your computer, but remember to update ``requirements.txt`` and explain the environment changes in your notebook (no penalty for this if necessary).\n","\n","### **Option 2: using Google Colab**\n","If you are using Google Colab notebook for this assignment, you will need to run a few commands to set up our environment on Google Colab, as shown below:\n","    \n","</div>"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"nSynqY5xfmaV","executionInfo":{"status":"ok","timestamp":1712433323968,"user_tz":-120,"elapsed":3,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}}},"outputs":[],"source":["# This cell makes sure modules are auto-loaded when you change external python files\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"VfVHqiSvK1aB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712433369657,"user_tz":-120,"elapsed":15430,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"60f4399d-e794-4914-9764-8d4d23f28332"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/a2-2024-mimimamalah\n"]}],"source":["# If you are working in Colab, then consider mounting your assignment folder to your drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Direct to your assignment folder.\n","%cd /content/drive/MyDrive/a2-2024-mimimamalah"]},{"cell_type":"markdown","metadata":{"id":"2LcGSuvWtmPf"},"source":["Install packages that are not included in the Colab base envrionemnt:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"SFXMx5FXtZhQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712402063166,"user_tz":-120,"elapsed":14157,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"4e94606c-82d4-414f-f463-8960a4bf35cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0)\n","Requirement already satisfied: transformers==4.38.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.38.1)\n","Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.8.1)\n","Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.2.2)\n","Requirement already satisfied: huggingface-hub==0.20.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.20.3)\n","Requirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.25.2)\n","Requirement already satisfied: tqdm==4.66.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.66.2)\n","Requirement already satisfied: scipy==1.11.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.11.4)\n","Requirement already satisfied: urllib3==2.0.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.0.7)\n","Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.16.0)\n","Requirement already satisfied: tensorboard==2.15.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.15.2)\n","Requirement already satisfied: jsonlines==4.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.13.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (2.18.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (12.1.105)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (2.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (0.4.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r requirements.txt (line 3)) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r requirements.txt (line 3)) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->-r requirements.txt (line 4)) (3.4.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (1.62.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (3.6)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (3.0.2)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines==4.0.0->-r requirements.txt (line 12)) (23.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->-r requirements.txt (line 1)) (12.4.127)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 11)) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 11)) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 11)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard==2.15.2->-r requirements.txt (line 11)) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.1->-r requirements.txt (line 2)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.1->-r requirements.txt (line 2)) (3.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.1->-r requirements.txt (line 2)) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.15.2->-r requirements.txt (line 11)) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->-r requirements.txt (line 1)) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 11)) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard==2.15.2->-r requirements.txt (line 11)) (3.2.2)\n"]}],"source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # limiting to one GPU\n","\n","# Install dependencies\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"NkG_cTYvfmaa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712402068263,"user_tz":-120,"elapsed":5102,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"e692cf10-15c9-4f63-924e-66094337b961"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7b2e680dcc50>"]},"metadata":{},"execution_count":4}],"source":["import numpy as np\n","import jsonlines\n","import random\n","\n","import torch\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","\n","# TODO: Enter your Sciper number\n","SCIPER = '329571'\n","seed = int(SCIPER)\n","torch.backends.cudnn.deterministic = True\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"XojPj_sqfmab","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712402068263,"user_tz":-120,"elapsed":4,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"2e6596cf-5b97-4d33-e1b7-589453c5b7be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Good to go!\n"]}],"source":["# Check the availability of GPU (proceed only it returns True!)\n","if torch.cuda.is_available():\n","  print('Good to go!')\n","else:\n","  print('Please set GPU via Edit -> Notebook Settings.')"]},{"cell_type":"markdown","metadata":{"id":"rHhgkhaH-IUl"},"source":["<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n","    \n","# PART 1: Sentiment Analysis (33 pts)\n","\n","In this part, we will finetune a pretrained language model (Roberta) on sentiment analysis(SA) task.\n","\n","> Specifically, we will focus on a binary sentiment classification task for multi-domain product reviews. It requires the model to **classify a given paragraph of review by its sentiment polarity (positive or negative)**.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"tD2YPuqeIYBN"},"source":["### Load Training Dataset (`train_sa.jsonl`)\n","\n","**You can run the following cell to have the first glance at your data**. Each data sample is a python dictionary, which consists of following components:\n","- input review (*'review'*): a natural language sentence or a paragraph commenting about a product.\n","- domain (*'domain'*): describing the type of product being reviewed.\n","- label of sentiment (*'label'*): indicating whether the review states positive or negative views about the product."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1711480764310,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"},"user_tz":-60},"id":"p-ODgcNUqYtm","outputId":"3cdd337d-b360-4992-f8ea-2780233efa26"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'review': \"THis book was horrible.  If it was possible to rate it lower than one star i would have.  I am an avid reader and picked this book up after my mom had gotten it from a friend.  I read half of it, suffering from a headache the entire time, and then got to the part about the relationship the 13 year old boy had with a 33 year old man and i lit this book on fire.  One less copy in the world...don't waste your money. I wish i had the time spent reading this book back so i could use it for better purposes.  THis book wasted my life\", 'domain': 'books', 'label': 'negative'}\n","{'review': 'Sphere by Michael Crichton is an excellant novel. This was certainly the hardest to put down of all of the Crichton novels that I have read. The story revolves around a man named Norman Johnson. Johnson is a phycologist. He travels with 4 other civilans to a remote location in the Pacific Ocean to help the Navy in a top secret misssion. They quickly learn that under the ocean is a half mile long spaceship. The civilans travel to a center 1000 feet under the ocean to live while researching the spacecraft. They are joined by 5 Navy personel to help them run operations. However on the surface a typhoon comes and the support ships on the surface must leave. The team of ten is stuck 1000 feet under the surface of the ocean. After a day under the sea they find out that the spacecraft is actually an American ship that has explored black holes and has brought back some strange things back to earth. This novel does not have the research that some of the other Crichton novels have, but it still has a lot of information on random things from the lawes of partial pressure to behavior analysis. I would strongly recommend this book', 'domain': 'books', 'label': 'positive'}\n","{'review': \"This entire movie could have run in only 20 minutes and you wouldn't miss anything and might even enjoy it. Unfortunately it ran 88 minutes too long and I couldn't wait for it to end.  I saw it in the theater and the people all around me were all complaining how boring it was. At least a quarter of them walked out before the end. It's that bad. It's a shame, I love a good suspense/horror movie and the decent actors in this movies were waisted\", 'domain': 'dvd', 'label': 'negative'}\n","{'review': \"I'm not sure why Sony, which now owns I Dream of Jeannie, decided to colorize the first season of this series.  Whatever the reason, you can readily tell by looking at the prices here on Amazon.com that the original black-and-white version of the first season is worth a lot more.  The reason for that is simple--I Dream of Jeannie was originally broadcast in black-and-white.  And for a television fan like myself, that's the ONLY way to watch the first season. The episodes themselves are just as I remember seeing them.  Since I wasn't around in 1965, I'm pretty sure I've never seen these without the cuts that have been referenced here.  But to me, they're still pretty good.  The theme music, in my opinion, is every bit as good as the second theme, introduced when Jeannie went to color in 1966. The one thing that truly will drive the purists nuts is the fact that Sony stripped off the old Screen Gems animation from the end of every episode.  That logo was attached to so many classic shows from the 1960s and 1970s, and it is consistenly rated, along with Viacom's old blue V of Doom, as the scariest logo in the history of television.  The new Sony outro doesn't pack the same punch. Still, if you liked Jeannie way back when, you'll love it now, especially since you can watch it anytime you like, without commercial interruption\", 'domain': 'dvd', 'label': 'positive'}\n","{'review': 'cons tips extremely easy on carpet and if you have a lot of cds stacked at the top poorly designed, it is a vertical cd rack that doesnt have individual slots for cds, so if you want a cd from the bottom of a stack you have basically pull the whole stack to get to it putting it together was a pain, the one i bought i had to break a piece of metal just to fit it in its guide holes. again..poorly designed... doesnt even fit cds that well, there are gaps, and the cd casses are loose fitting pros .......... i guess it can hold a lot of cds....', 'domain': 'electronics', 'label': 'negative'}\n","{'review': 'I purchased this unit due to frequent blackouts in my area and 2 power supplies going bad.  It will run my cable modem, router, PC, and LCD monitor for 5 minutes.  This is more than enough time to save work and shut down.   Equally important, I know that my electronics are receiving clean power. I feel that this investment is minor compared to the loss of valuable data or the failure of equipment due to a power spike or an irregular power supply. As always, Amazon had it to me in &lt;2 business days', 'domain': 'electronics', 'label': 'positive'}\n","{'review': \"He just looks away from where the spray emits--and barks again! It also doesn't work 100% of the time...and we're not sure why.  When we fill it, it seems to work fairly well right after but it either does not have as many sprays as it is supposed to, or it isn't working very long. It does work well for my other small dog who is not such a persistent barker.  Terriers are just too stubborn to care if they're getting sprayed, I guess.\", 'domain': 'housewares', 'label': 'negative'}\n","{'review': 'For those of you unfamiliar with the \"A Series of Unfortunate Events\" books, they detail the absurdly tragic lives of the fictional Baudelaire orphans, and their struggles to overcome adversity (after adversity, after adversity, ad infinitum).  Hovering in the background of their tragedies and occasional, brief glimpses of happiness, is their distant cousin, the scheming, greedy, conniving, ruthless cousin, the Count. Given the concept of the story, this poster fits perfectly, with the orphans, standing together but otherwise alone, in the middle, with the shadow of the Count casting gloom upon them.  It is a perfect fit, as is the casting of Jim Carrey as the Count', 'domain': 'housewares', 'label': 'positive'}\n"]}],"source":["data_dir = 'data'\n","data_train_path = os.path.join(data_dir, 'train_sa.jsonl')\n","with jsonlines.open(data_train_path, \"r\") as reader:\n","    for sid, sample in enumerate(reader.iter()):\n","        if sid % 200 == 0:\n","            print(sample)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":372,"referenced_widgets":["26ca3a98423d41a4bbce05d41bd42702","34a4a962d6204853b763cfc5d301c85c","d222118099144ead9cc38e2f4219e275","30495395af374e1bb0a6b15930a63ba2","8ddb20dbe27a408c930846d8d829fced","8455e18213ab46689ed279898e63e977","d75134905e8448e68b992bd695e8e214","9b5ebefb2b1e4dad8a0e33d057998886","246b33abb3524ee28a91044ba34e1819","3ae037058b4e480d8173b7a567296aed","d0f4d2b2c4464198bd6223deb9fa13e4","d23aed683334472485b7d11704428885","ef7f23a0cd5f45bebeb678152f0d60a6","39f8398b3b56470b85316e96f329e1e3","4eb54466a61b42c4b20490ccbb42b406","7cdf0b6e820e44998b394481cae9267b","d90ae1031ca847359418bf24d505a2ea","4c041f02fe2b4f28a011a6288700d931","96ccb3fd3df74e2284ec48d02ac5cc83","c73d4b041cac4c909737eb8b5b34f162","eb291126b4974732af114751db8e70f6","79538cf1d0694db2a386f76b58c7e514","48de2470b898473aa82ea4fd4fb4e167","1dfdf36317f542aab0d1cbba07c61390","a97de606ffa94efda1aa087c37a7884b","d9205800b28d4d81bffc319fe0c6d482","a726addf54a14ce48fb5fc0d75bc672b","7d48d8de98db422f8e882b7801057875","b39358b196e141a7847641bd8043fc6d","2546b8f6abc0471a9fbce9be7efcac47","5219176ea7bf4684be6a034cf1e6cda6","ba6a13612af24266a1e97371810594ce","f09c7589deeb4e75ace35a91ebb02f7f","5c37f5cd2c384650ae04ad6a221f24e5","218825ebc9eb47eeb686cc66f905229d","041a5c9907bb4cb3883cf5cea5f821e9","d9a930cf63e748bc927ddc333c0b280c","64e06f46aeb04472b5696ed6e7ba8268","786e3889321c43b4862e774dfb837e63","5811ae4956ed4e2aa7798d02807b5a65","f128535cb1614c0fb12e386947ee826d","eaf8593454ff47f18aa298ddbff6b41d","5dd960565d4b40e8b59b1a5d418e9a1c","17d974f8644f411da416f3c6a0a6581e","ca5d77cf8fb048caad685d591d1d4590","c6abfef27b5b4f52972045d9cefcaceb","6d654b3209094a1e9c3343243eaa2022","7e58a3c447604e688804f4521a88343d","0ab5b39f4883459e913ff5a585f541c7","0ce0adf80b86495aa317189faa734347","d6336026bba24b4b87668e131b836e45","21d3b619fe9d48c7b34f38db67e6e24d","afdd51d6ed634f959bad51ee7a1a225a","571e69ac328046e0a84a6f5ac99f0711","fa8a0666b9e244be9e03d052bd32fbbf","aa45b9b8555f44f5834a67966b43f317","eb568c468b194009b42a03334214378c","c986f050552a47dc9e45e218796c73ab","d144f362705149648bbcb79b88ab3269","9861b0b1732b48418d3128f41be248bb","00f51ab450b449d2906e2901c05c6d9a","a18e182295944db79419ae71963a69ae","755415e3599446e2825770d50eac34b3","391f62f003994c6da88a89abf4a51f83","8f1c019906a9493783b176accb3a733b","b8bca4aa51c3493886a0d5dbb4ee858b"]},"executionInfo":{"elapsed":16074,"status":"ok","timestamp":1711480780382,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"},"user_tz":-60},"id":"BvM8jd_3QObg","outputId":"739e5552-89c2-46c1-e484-dde64489395e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26ca3a98423d41a4bbce05d41bd42702"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23aed683334472485b7d11704428885"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48de2470b898473aa82ea4fd4fb4e167"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c37f5cd2c384650ae04ad6a221f24e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca5d77cf8fb048caad685d591d1d4590"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa45b9b8555f44f5834a67966b43f317"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# We use the following pretrained tokenizer and model\n","model_name = \"FacebookAI/roberta-base\"\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)"]},{"cell_type":"markdown","metadata":{"id":"fCETOFT2dB4u"},"source":["## ðŸŽ¯ Q1.1: **Dataset Processing (10 pts)**\n","\n","Our first step is to constructing a Pytorch Dataset for SA task. Specifically, we will need to implement **tokenization** and **padding** using a HuggingFace pre-trained tokenizer.\n","\n","**TODOðŸ”»: Complete `SADataset` class following the instructions in `sa.py`, and test by running the following cell.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17728,"status":"ok","timestamp":1711480798092,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"},"user_tz":-60},"id":"_5Sya9W5BTDl","outputId":"15eb00e6-cb24-4c81-dd6a-cfabadd608b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Building SA Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["1600it [00:05, 297.60it/s] \n"]}],"source":["from sa import SADataset\n","model_name = \"FacebookAI/roberta-base\"\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","dataset = SADataset(\"data/train_sa.jsonl\", tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3950,"status":"ok","timestamp":1711480802040,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"},"user_tz":-60},"id":"K1hOkTuffmae","outputId":"16f3cb69-a089-42ec-a439-5f688e011d79"},"outputs":[{"output_type":"stream","name":"stdout","text":["SADataset test correct âœ…\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["from testA2 import test_SADataset\n","test_SADataset(dataset)"]},{"cell_type":"markdown","metadata":{"id":"W0weQpG6_3vO"},"source":["## ðŸŽ¯ Q1.2: **Model Training and Evaluation (18 pts)**\n","\n","Next, we will implement the training and evaluation process to finetune the model.\n","\n","- For training: you will need to calculate the **loss** and update the model weights by using **Adam optimizer**. Additionally, we add a **learning rate schedular** to adopt an adaptive learning rate during the whole training process.\n","\n","- For evaluation: you will need to compute the **confusion matrix** and **F1 scores** to assess the model performance.\n","\n","**TODOðŸ”»: Complete the `compute_metrics()`, `train()` and `evaluate()` functions following the instructions in the `sa.py` file, you can test compute_metrics() by running the following cell.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":574,"status":"ok","timestamp":1711480802599,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"},"user_tz":-60},"id":"6w7Leraw4tIY","outputId":"fe0a583b-1912-47a5-ded4-fcc13cb84af9"},"outputs":[{"output_type":"stream","name":"stdout","text":["compute_metric test correct âœ…\n"]}],"source":["from sa import compute_metrics, train, evaluate\n","\n","from testA2 import test_compute_metrics\n","test_compute_metrics(compute_metrics)"]},{"cell_type":"markdown","metadata":{"id":"pvCUS748_3vS"},"source":["#### **Start Training and Validation!**\n","\n","TODOðŸ”»: (1) [coding question] Train the model with the following two different learning rates (other hyperparameters should be kept consistent).\n","\n","> A. learning_rate = 1e-5\n","\n","> B. learning_rate = 2e-5\n","\n","**Note:** *Each training will take ~7-10 minutes using a T4 Colab GPU.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2008,"status":"ok","timestamp":1711481277518,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"},"user_tz":-60},"id":"zn66mMOj_3vS","outputId":"65856aed-dd06-41f6-e31b-34b5e4e07dcb"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_name = \"FacebookAI/roberta-base\"\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","model.to(device)\n","\n","batch_size = 8\n","epochs = 4\n","max_grad_norm = 1.0\n","warmup_percent = 0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":187767,"status":"ok","timestamp":1711481473720,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"},"user_tz":-60},"id":"Z5aWqR1h_3vS","outputId":"2bc4eb56-9650-47c3-f7f5-902301946db9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Building SA Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["1600it [00:03, 437.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Building SA Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["6400it [00:08, 786.68it/s] \n","Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 12.51it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:24<00:00, 33.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0 | Training Loss: 0.697 | Validation Loss: 0.684\n","Epoch 0 SA Validation:\n","Confusion Matrix:\n","[[ 854 2346]\n"," [ 606 2594]]\n","F1: (36.65%, 63.73%) | Macro-F1: 50.19%\n","Model Saved!\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:23<00:00,  8.36it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:17<00:00, 45.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Training Loss: 0.492 | Validation Loss: 0.384\n","Epoch 1 SA Validation:\n","Confusion Matrix:\n","[[2689  511]\n"," [ 193 3007]]\n","F1: (88.42%, 89.52%) | Macro-F1: 88.97%\n","Model Saved!\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:20<00:00,  9.72it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:24<00:00, 32.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 2 | Training Loss: 0.317 | Validation Loss: 0.311\n","Epoch 2 SA Validation:\n","Confusion Matrix:\n","[[2874  326]\n"," [ 284 2916]]\n","F1: (90.41%, 90.53%) | Macro-F1: 90.47%\n","Model Saved!\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:19<00:00, 10.50it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:17<00:00, 45.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 3 | Training Loss: 0.175 | Validation Loss: 0.408\n","Epoch 3 SA Validation:\n","Confusion Matrix:\n","[[3004  196]\n"," [ 401 2799]]\n","F1: (90.96%, 90.36%) | Macro-F1: 90.66%\n","Model Saved!\n"]}],"source":["learning_rate = 1e-5  # play around with this hyperparameter\n","\n","# On Ed train models on data/train_sa.jsonl,\n","# and evaluate on data/test_sa.jsonl\n","train_dataset = SADataset('data/train_sa.jsonl', tokenizer)\n","dev_dataset = SADataset('data/test_sa.jsonl', tokenizer)\n","\n","train(train_dataset=train_dataset,\n","      dev_dataset=dev_dataset,\n","      model=model,\n","      device=device,\n","      batch_size=batch_size,\n","      epochs=epochs,\n","      learning_rate=learning_rate,\n","      warmup_percent=warmup_percent,\n","      max_grad_norm=max_grad_norm,\n","      model_save_root='models/',\n","      tensorboard_path=\"./tensorboard/part1_lr{}\".format(learning_rate))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5buT9IULi93","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711481709038,"user_tz":-60,"elapsed":158230,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"8d035ba4-145e-4098-9550-08ba118e4bf5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Building SA Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["1600it [00:01, 1015.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Building SA Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["6400it [00:07, 805.33it/s] \n","Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:16<00:00, 12.04it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:18<00:00, 43.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0 | Training Loss: 0.121 | Validation Loss: 0.492\n","Epoch 0 SA Validation:\n","Confusion Matrix:\n","[[3006  194]\n"," [ 429 2771]]\n","F1: (90.61%, 89.89%) | Macro-F1: 90.25%\n","Model Saved!\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:19<00:00, 10.45it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:21<00:00, 37.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Training Loss: 0.093 | Validation Loss: 0.670\n","Epoch 1 SA Validation:\n","Confusion Matrix:\n","[[2970  230]\n"," [ 411 2789]]\n","F1: (90.26%, 89.69%) | Macro-F1: 89.98%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:16<00:00, 12.41it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:17<00:00, 46.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 2 | Training Loss: 0.095 | Validation Loss: 0.625\n","Epoch 2 SA Validation:\n","Confusion Matrix:\n","[[2894  306]\n"," [ 317 2883]]\n","F1: (90.28%, 90.25%) | Macro-F1: 90.27%\n","Model Saved!\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:16<00:00, 12.07it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:17<00:00, 46.18it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch: 3 | Training Loss: 0.037 | Validation Loss: 0.893\n","Epoch 3 SA Validation:\n","Confusion Matrix:\n","[[2698  502]\n"," [ 207 2993]]\n","F1: (88.39%, 89.41%) | Macro-F1: 88.90%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# I don't know if we should reload the roberta model or not.\n","\n","learning_rate = 2e-5  # play around with this hyperparameter\n","\n","# On Ed train models on data/train_sa.jsonl,\n","# and evaluate on data/test_sa.jsonl\n","train_dataset = SADataset('data/train_sa.jsonl', tokenizer)\n","dev_dataset = SADataset('data/test_sa.jsonl', tokenizer)\n","\n","train(train_dataset=train_dataset,\n","      dev_dataset=dev_dataset,\n","      model=model,\n","      device=device,\n","      batch_size=batch_size,\n","      epochs=epochs,\n","      learning_rate=learning_rate,\n","      warmup_percent=warmup_percent,\n","      max_grad_norm=max_grad_norm,\n","      model_save_root='models/',\n","      tensorboard_path=\"./tensorboard/part1_lr{}\".format(learning_rate))"]},{"cell_type":"markdown","metadata":{"id":"RyuWYQbMfmah"},"source":["TODOðŸ”»: (2) [textual question] compare and discuss the results.\n","\n","- Which learning rate is better?\n","\n","\n","\n","- For learning rate 1e-5:\n","  * Training Loss: ~0.175\n","  * Validation Loss: ~0.408\n","  * Macro-F1 Score: ~90.66%\n","  * Confusion Matrix:\n","\n","    [[3004   196]\n","\n","    [401     2799]]\n","\n","- For learning rate 2e-5:\n","  * Training Loss: ~0.093\n","  * Validation Loss: ~0.670\n","  * Macro-F1 Score: ~90.25%\n","  * Confusion Matrix:\n","\n","    [[2698     502]\n","\n","    [207      2993]]\n","\n","- The model with a learning rate of 1e-5 has a higher training loss but a significantly lower validation loss compared to the 2e-5 model. This suggests that the 1e-5 model is generalizing better to the validation set, which is often better to ensure they perform well on unseen data.\n","\n","- The 2e-5 model has a lower training loss which might indicate it is fitting the training data better, but this comes at the cost of a higher validation loss, potentially indicating overfitting to the training data.\n","\n","- The Macro-F1 Scores are very similar, with the 1e-5 model slightly outperforming the 2e-5 model. Given that F1 is an important measure of a model's performance, especially in class-imbalanced datasets, the 1e-5 learning rate seems to be the better choice.\n","\n","#### Confusion Matrix Comparison\n","*   True Positives (TP): At 1e-5, the model has more true positives, meaning it correctly identifies positive cases more often than at 2e-5.\n","*   False Negatives (FN): At 1e-5, the model has fewer false negatives, which is better as it means fewer positive cases are being missed.\n","*   False Positives (FP): At 2e-5, the model has fewer false positives, indicating it's better at not misclassifying negative cases as positive.\n","*   True Negatives (TN): Conversely, at 2e-5, the model has more true negatives, meaning it correctly identifies negative cases more often than at 1e-5.\n","\n","=> The model trained with a 1e-5 learning rate is better at identifying positive cases correctly (higher TP, lower FN). The model with a 2e-5 learning rate is better at avoiding false alarms (lower FP) and identifying negative cases (higher TN).\n"]},{"cell_type":"markdown","metadata":{"id":"wGuzGJCB_3vT"},"source":["## ðŸŽ¯ Q1.3: **Fine-Grained Validation (5 pts)**\n","\n","TODOðŸ”»: (1) [coding question] Use the model checkpoint trained from the first learning_rate setting (lr=1e-5), check the model performance on each domain subsets of the validation set. You should report **the validation loss**, **confusion matrix**, **F1 scores** and **Macro-F1 on each domain**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YCWWJjTP_3vT"},"outputs":[],"source":["# Split the test sets into subsets with different domains\n","# Save the subsets under 'data/'\n","# Replace \"...\" with your code\n","domain_data = {}\n","data_dir = 'data'\n","data_train_path = os.path.join(data_dir, 'train_sa.jsonl')\n","with jsonlines.open(data_train_path, \"r\") as reader:\n","    for sample in reader.iter():\n","        if sample['domain'] not in domain_data:\n","            domain_data[sample['domain']] = []\n","        domain_data[sample['domain']].append(sample)\n","\n","for domain, samples in domain_data.items():\n","    with jsonlines.open(\"data/test_sa_\"+domain+\".jsonl\", mode=\"w\") as writer:\n","        for sd in samples:\n","            writer.write(sd)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4J2pu60xHTd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711483904314,"user_tz":-60,"elapsed":9367,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"5afa3109-43b9-4415-e5ab-4db63643fd4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Building SA Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["400it [00:01, 375.66it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:01<00:00, 42.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Domain: books\n","Validation Loss: 0.080\n","Confusion Matrix:\n","[[198   2]\n"," [  6 194]]\n","F1: (98.02%, 97.98%) | Macro-F1: 98.00%\n","Building SA Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["400it [00:01, 292.64it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:01<00:00, 45.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Domain: dvd\n","Validation Loss: 0.083\n","Confusion Matrix:\n","[[200   0]\n"," [  8 192]]\n","F1: (98.04%, 97.96%) | Macro-F1: 98.00%\n","Building SA Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["400it [00:00, 1008.61it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:01<00:00, 43.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Domain: electronics\n","Validation Loss: 0.065\n","Confusion Matrix:\n","[[196   4]\n"," [  2 198]]\n","F1: (98.49%, 98.51%) | Macro-F1: 98.50%\n","Building SA Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["400it [00:00, 536.96it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:01<00:00, 42.63it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Domain: housewares\n","Validation Loss: 0.018\n","Confusion Matrix:\n","[[199   1]\n"," [  1 199]]\n","F1: (99.50%, 99.50%) | Macro-F1: 99.50%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["learning_rate = 1e-5\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","checkpoint_path = 'models/lr1e-05-warmup0.3'\n","tokenizer = RobertaTokenizer.from_pretrained(checkpoint_path)\n","model = RobertaForSequenceClassification.from_pretrained(checkpoint_path)\n","model.to(device)\n","\n","results_save_dir = 'predictions/'\n","\n","# Evaluate and save prediction results in each domain\n","# Replace \"...\" with your code\n","for domain in domain_data.keys():\n","\n","    dev_dataset = SADataset(f\"data/test_sa_{domain}.jsonl\", tokenizer)\n","\n","    dev_loss, confusion, f1_pos, f1_neg = evaluate(dev_dataset, model, device, batch_size,\n","                                                   result_save_file='predictions/test_'+domain+'.jsonl')\n","    macro_f1 = (f1_pos + f1_neg) / 2\n","\n","    # It bothers me the fact that the domain does not in a new line\n","    print(f'\\nDomain: {domain}')\n","    #print(f'Domain: {domain}')\n","    print(f'Validation Loss: {dev_loss:.3f}')\n","    print(f'Confusion Matrix:')\n","    print(confusion)\n","    print(f'F1: ({f1_pos*100:.2f}%, {f1_neg*100:.2f}%) | Macro-F1: {macro_f1*100:.2f}%')"]},{"cell_type":"markdown","metadata":{"id":"kId7_FaEfmai"},"source":["TODOðŸ”»: (2) [textual question] compare and discuss the results.\n","\n","**Questions:**\n","- On which domain does the model perform the best? the worst?\n","- Give some possible explanations of why the model's best-performed domain is easier, and why the model's worst-performed domain is more challenging. Use some examples to support your explanations.\n","\n","**Note:** To find examples for supporting your discussion, save the model prediction results on each domain under the `predictions/` folder, by specifying the `result_save_file` parameter in the *evaluate* function."]},{"cell_type":"markdown","metadata":{"id":"p8j7l7mrS2iH"},"source":["- **Books**\n","  - Validation Loss: 0.080\n","  - Confusion Matrix: High true positives (198) and true negatives (194), very few false positives (2) and false negatives (6).\n","  - F1 Score: (98.02%, 97.98%).\n","  - Macro-F1: 98.00%.\n","\n","- **DVD**\n","  - Validation Loss: 0.083\n","  - Confusion Matrix: Perfect true positives (200), high true negatives (192), no false positives (0), and some false negatives (8).\n","  - F1 Score: (98.04%, 97.96%).\n","  - Macro-F1: 98.00%.\n","\n","- **Electronics**\n","  - Validation Loss: 0.065\n","  - Confusion Matrix: High true positives (196) and true negatives (198), few false positives (4) and very few false negatives (2).\n","  - F1 Score: (98.49%, 98.51%).\n","  - Macro-F1: 98.50%.\n","\n","- **Housewares**\n","  - Validation Loss: 0.018\n","  - Confusion Matrix: Nearly perfect true positives (199) and true negatives (199), very few false positives (1) and false negatives (1).\n","  - F1 Score: (99.50%, 99.50%).\n","  - Macro-F1: 99.50%.\n","\n","**Performance Comparison:**\n","- The model performs the **best on the housewares domain**, with the highest F1 scores and Macro-F1, and the lowest validation loss.\n","- The model performs the **worst on the books domain**, though it's still strong.\n","\n","**Possible Explanations:**\n","- **Housewares domain**:\n","\n","  *   Clear Indicators of Discontent: The language used in the housewares reviews tends to be very clear and direct about the issues the customers faced. Phrases like \"cheaply made,\" \"did not work,\" \"completely useless,\" \"burned ourselves,\" and \"horrible piece of junk\" are unambiguous and strong indicators of negative sentiment.\n","  *  Specificity and Detail: These reviews are detailed and specific about the product's issues. They don't just express dissatisfaction; they explain the problems encountered with the products.\n","  * Less Subjectivity and Nuance: Unlike book or movie reviews, which can be highly subjective and nuanced, housewares reviews tend to be more objective. A product either performs its function or doesn't, which is a simpler for a model to interpret.\n","  *  Consistent Vocabulary: The vocabulary related to housewares problems is consistent (\"faulty\"). Once the model learns these indicators of negative sentiment, it can apply them to unseen reviews in the same domain.\n","\n","- **Books domain**:\n","\n"," *  Subjective Content: The book reviews include emotional expressions (\"horrible,\" \"headache the entire time,\" \"lit this book on fire\"). They often reflect a reader's personal and subjective experience. This subjectivity can include mixed feelings.\n"," *  Elaborate Narratives: Reviews in the \"books\" domain can contain elaborate narratives that explain a reader's thought process, which might not have clearly defined sentiment cues, making it harder for the model.\n"," *  Cultural and Contextual References: Understanding and interpreting reviews often require cultural or literary context. For example,sarcasm, or metaphors might be misinterpreted without deeper knowledge.\n"," *  Varying Detail: Book reviews can be long and include a level of detail and critique that is specific to literature. The model must understand all of this information to predict the sentiment.\n"," *  Critique of Authorial Intent: Some negative reviews may critique the author rather than the content of the book, which could be challenging for a model if it has been trained to focus on product functionality rather than personal opinions about the creator.\n"]},{"cell_type":"markdown","metadata":{"id":"6NyMZ5E4-QxM"},"source":["<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n","\n","# PART 2: Identify Model Shortcuts (22 pts)\n","\n","In this part, We aim to find out the shortcut features learnt by the sentiment analysis model we have trained in Part1. We will be using the model checkpoint trained with `learning rate=1e-5`.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"4lCHLdaH_3vT"},"source":["## ðŸŽ¯ Q2.1: **N-gram Pattern Extraction (6 pts)**\n","We hypothesize that `n-gram`s could be the potential shortcut features learnt by the SA model. An `n-gram` is defined as a sequence of n consecutive words appeared in a natural language sentence or paragraph.\n","\n","Thus, we aim to extract that an n-gram that appears in a review may serve as a key indicator of the polarity of the review's sentiment, for example:\n","\n",">- **Review 1**: This book was **horrible**. If it was possible to rate it **lower than one star** I would have.\n",">- **Review 2**: **Excellent** book, **highly recommended**. Helps to put a realistic perspective on millionaires.\n","\n","For Review 1, the `1-gram \"horrible\"` and the `4-gram \"lower than one star\"` serve as two key indicators of negative sentiment. While for Review 2, the `1-gram \"excellent\"` and the `2-gram \"highly recommended\"` obviously indicate positive sentiment."]},{"cell_type":"markdown","metadata":{"id":"_NovYRxv_3vU"},"source":["TODOðŸ”»: (1) [coding question] Complete `ngram_extraction()` function in `shortcut.py` file.\n","\n","The returned *ngrams* contains a **list** of dictionaries. The `n-th` **dictionary** corresponds the `n-grams` (n=1,2, 3, 4).\n","\n","The keys of each dictionary should be a **unique n-gram string** appeared in reviews, and the value of each n-gram key records the frequency of positive/negative predictions **made by the model** when the n-gram appears in the review, i.e., `\\[#positive_predictions, #negative_predictions\\]`.\n","\n","> Example: **`ngrams`[0]['horrible'][0]** should return the number of the positive predictions made by the model when the 1-gram token 'horrible' appear in the given review. i.e., \\[#positive_predictions, #negative_predictions\\].\n","\n","**Note:** (1) All the sequences contain punctuations should NOT be counted as a n-gram (e.g. `it is great .` is NOT a 4-gram, but `it is great` is a 3-gram); (2) All stop-words should NOT be counted as 1-grams, but can appear in other n-gram sequences (e.g. `is` is NOT a 1-gram token, but `it is great` can be a 3-gram token.)"]},{"cell_type":"markdown","metadata":{"id":"UTHt1frZ_3vU"},"source":["## ðŸŽ¯ Q2.2: **Distill Potentially Useful Patterns (8 pts)**\n","\n","TODOðŸ”»: (2) [coding question] For each group of n-grams (n=1,2,3,4), find and **print** the **top-100 n-gram sequences** with the **greatest frequency of appearance**, which could contain frequent semantic features and would be used as our feature list."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFZm6tFJFmlg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711566967930,"user_tz":-60,"elapsed":2236,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"c8129b95-5747-4cde-a50f-b398ae5db37c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["from shortcut import ngram_extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUyal5mW_3vU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711566988815,"user_tz":-60,"elapsed":20887,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"78e582cb-70cf-45f8-927b-827ef015fbe6"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:07<00:00, 52.82it/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:02<00:00, 163.24it/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:02<00:00, 173.73it/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:05<00:00, 72.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Top-100 most frequent 1-grams:\n","[('one', [487, 552]), ('book', [461, 489]), ('like', [294, 329]), ('would', [260, 329]), ('movie', [212, 341]), ('good', [265, 252]), ('time', [217, 285]), ('even', [157, 276]), ('great', [290, 133]), ('well', [247, 173]), ('get', [176, 235]), ('film', [221, 172]), ('much', [163, 214]), ('first', [187, 189]), ('read', [154, 203]), ('really', [170, 173]), ('use', [181, 160]), ('also', [203, 137]), ('j', [165, 150]), ('work', [136, 171]), ('story', [159, 128]), ('many', [157, 128]), ('people', [114, 163]), ('make', [118, 159]), ('way', [145, 131]), ('could', [95, 172]), ('us', [128, 129]), ('better', [107, 148]), ('two', [126, 128]), ('2', [95, 156]), ('h', [127, 118]), ('new', [138, 103]), ('little', [132, 107]), ('r', [117, 118]), ('used', [111, 119]), ('l', [128, 100]), ('back', [90, 134]), ('b', [124, 99]), ('man', [127, 95]), ('product', [84, 137]), ('vd', [121, 97]), ('never', [96, 118]), ('made', [105, 108]), ('p', [131, 82]), ('3', [103, 107]), ('love', [148, 61]), ('know', [88, 121]), ('best', [139, 70]), ('buy', [98, 109]), ('c', [95, 110]), ('w', [120, 85]), ('think', [92, 113]), ('g', [120, 84]), ('go', [101, 102]), ('1', [88, 113]), ('years', [106, 95]), ('see', [102, 98]), ('life', [128, 72]), ('e', [117, 78]), ('another', [80, 112]), ('able', [96, 96]), ('bought', [80, 109]), ('f', [87, 100]), ('still', [91, 95]), ('want', [87, 98]), ('k', [110, 75]), ('find', [102, 82]), ('every', [93, 86]), ('something', [78, 97]), ('got', [54, 119]), ('quality', [84, 89]), ('however', [79, 89]), ('thing', [52, 116]), ('old', [84, 83]), ('er', [93, 74]), ('bad', [34, 130]), ('say', [67, 95]), ('found', [83, 79]), ('characters', [62, 99]), ('set', [95, 65]), ('money', [53, 105]), ('need', [75, 81]), ('long', [79, 76]), ('since', [71, 84]), ('v', [71, 83]), ('right', [66, 88]), ('n', [78, 75]), ('5', [70, 82]), ('end', [72, 79]), ('recommend', [89, 61]), ('makes', [85, 64]), ('ing', [82, 67]), ('ever', [79, 70]), ('take', [76, 73]), ('thought', [63, 85]), ('works', [88, 60]), ('ed', [72, 75]), ('ly', [66, 81]), ('put', [74, 73]), ('world', [80, 66])]\n","Top-100 most frequent 2-grams:\n","[('ip od', [28, 51]), ('qu ot', [40, 28]), ('much better', [15, 27]), ('first time', [24, 18]), ('p aul', [31, 10]), ('highly recommend', [34, 3]), ('ch ina', [25, 10]), ('every time', [17, 18]), ('pan asonic', [14, 18]), ('customer service', [10, 21]), ('years ago', [14, 16]), ('j ames', [14, 15]), ('even though', [14, 15]), ('kitchen aid', [17, 12]), ('v ds', [10, 17]), ('j apan', [12, 14]), ('mar tha', [10, 15]), ('christ ian', [6, 19]), ('ro bert', [17, 8]), ('mem ore', [5, 20]), ('ore x', [5, 20]), ('h ollywood', [10, 14]), ('many people', [11, 12]), ('c ds', [10, 13]), ('long time', [14, 8]), ('high school', [15, 7]), ('h art', [13, 9]), ('sand isk', [5, 17]), ('year old', [7, 13]), ('ar ab', [1, 19]), ('would recommend', [14, 6]), ('ir aq', [19, 1]), ('therm ometer', [4, 16]), ('looks like', [7, 12]), ('john son', [19, 0]), ('sound quality', [6, 13]), ('cu isin', [9, 10]), ('ge orge', [11, 7]), ('5 stars', [9, 9]), ('every day', [15, 3]), ('ice cream', [7, 11]), ('works great', [14, 4]), ('mp 3', [12, 6]), ('isin art', [9, 9]), ('cast iron', [17, 1]), ('ever seen', [7, 10]), ('apan ese', [5, 12]), ('ann ie', [14, 3]), ('would make', [10, 7]), ('dish washer', [10, 7]), ('christ opher', [13, 3]), ('char lie', [10, 6]), ('high quality', [11, 5]), ('g iov', [15, 1]), ('jo e', [13, 3]), ('would like', [11, 5]), ('highly recommended', [15, 1]), ('make sure', [11, 5]), ('new one', [3, 13]), ('ear phones', [13, 3]), ('links ys', [2, 14]), ('le cre', [13, 3]), ('cre us', [13, 3]), ('us et', [13, 3]), ('ro ber', [3, 12]), ('ber ts', [3, 12]), ('j im', [7, 8]), ('really like', [11, 4]), ('gr ish', [8, 7]), ('works well', [11, 4]), ('gr ater', [5, 10]), ('py rex', [2, 13]), ('gal vest', [15, 0]), ('jack son', [3, 11]), ('go back', [6, 8]), ('da v', [0, 14]), ('rich ard', [4, 10]), ('dou glas', [3, 11]), ('many times', [6, 8]), ('b rit', [7, 7]), ('step hen', [4, 10]), ('well written', [12, 2]), ('well worth', [10, 4]), ('stainless steel', [9, 5]), ('car afe', [3, 11]), ('cal phal', [5, 9]), ('ast aire', [13, 0]), ('ke vin', [9, 4]), ('even worse', [0, 13]), ('look like', [4, 9]), ('ro th', [10, 3]), ('p eter', [8, 5]), ('tech support', [0, 13]), ('stopped working', [2, 11]), ('gig ab', [0, 13]), ('ab eat', [0, 13]), ('h oo', [6, 7]), ('good book', [8, 5]), ('good movie', [8, 4]), ('amer ican', [7, 5])]\n","Top-100 most frequent 3-grams:\n","[('mem ore x', [5, 20]), ('cu isin art', [9, 9]), ('j apan ese', [5, 12]), ('le cre us', [13, 3]), ('cre us et', [13, 3]), ('ro ber ts', [3, 12]), ('gig ab eat', [0, 13]), ('j es us', [3, 9]), ('j enn ifer', [7, 5]), ('b rit ish', [4, 7]), ('h oo ver', [6, 5]), ('le aph orn', [9, 2]), ('c ec ilia', [0, 10]), ('l u cy', [2, 8]), ('cr ich ton', [7, 2]), ('f iest aware', [4, 5]), ('k ru ps', [1, 8]), ('mand el bro', [0, 9]), ('g iov anna', [7, 1]), ('g iov anni', [8, 0]), ('w ahl berg', [6, 1]), ('w ither sp', [7, 0]), ('c asser ole', [6, 1]), ('mp 3 player', [2, 5]), ('would highly recommend', [7, 0]), ('ty ler per', [3, 3]), ('ler per ry', [3, 3]), ('christ ian ity', [1, 5]), ('ci code book', [0, 6]), ('j ess ica', [3, 3]), ('bro ck ovich', [0, 6]), ('rec om end', [4, 2]), ('del u ise', [0, 6]), ('miss iss ippi', [2, 4]), ('w w f', [0, 6]), ('r hod es', [4, 2]), ('co le man', [6, 0]), ('ither sp oon', [6, 0]), ('meat lo af', [6, 0]), ('step han ie', [1, 5]), ('never lo vel', [6, 0]), ('lo vel ier', [6, 0]), ('candy therm ometer', [0, 6]), ('25 g 0', [6, 0]), ('g 0 x', [6, 0]), ('freak onom ics', [0, 6]), ('cou l ter', [0, 6]), ('ne il g', [5, 1]), ('il g aiman', [5, 1]), ('cal v ino', [6, 0]), ('mag dal ene', [0, 5]), ('apan ese culture', [0, 5]), ('cl ic hed', [1, 4]), ('diss ap ointed', [0, 5]), ('ichael dou glas', [1, 4]), ('mark w ahl', [4, 1]), ('e instein videos', [1, 4]), ('j ess ie', [2, 3]), ('un plug ged', [2, 3]), ('ar n old', [2, 3]), ('nc ke ls', [1, 4]), ('dish washer safe', [3, 2]), ('kitchen aid mixer', [3, 2]), ('es press op', [2, 3]), ('press op ro', [2, 3]), ('step hen king', [1, 4]), ('ave mc ke', [3, 2]), ('rob iche aux', [2, 3]), ('l st oy', [5, 0]), ('f red ast', [4, 0]), ('red ast aire', [4, 0]), ('le dossier secrets', [0, 4]), ('rich ard le', [0, 4]), ('ard le igh', [0, 4]), ('h ink ley', [0, 4]), ('ge orge c', [3, 1]), ('bet ty h', [0, 4]), ('ty h utton', [0, 4]), ('gold bl um', [1, 3]), ('mon oton ous', [1, 3]), ('ro bert alt', [3, 1]), ('bert alt man', [3, 1]), ('ike e pps', [3, 1]), ('ess ica al', [1, 3]), ('ica al ba', [1, 3]), ('bro ok lyn', [2, 2]), ('eng ross ing', [2, 2]), ('would definitely recommend', [3, 1]), ('scar lett j', [3, 1]), ('lett j oh', [3, 1]), ('j oh ansson', [3, 1]), ('ach ik oma', [3, 1]), ('bob back lund', [0, 4]), ('back lund vs', [0, 4]), ('b ret h', [0, 4]), ('ret h art', [0, 4]), ('w f world', [0, 4]), ('agg riv ating', [0, 4]), ('r ick man', [0, 4]), ('ese w ither', [4, 0])]\n","Top-100 most frequent 4-grams:\n","[('le cre us et', [13, 3]), ('ty ler per ry', [3, 3]), ('w ither sp oon', [6, 0]), ('never lo vel ier', [6, 0]), ('25 g 0 x', [6, 0]), ('ne il g aiman', [5, 1]), ('j apan ese culture', [0, 5]), ('mark w ahl berg', [4, 1]), ('es press op ro', [2, 3]), ('f red ast aire', [4, 0]), ('rich ard le igh', [0, 4]), ('bet ty h utton', [0, 4]), ('ro bert alt man', [3, 1]), ('j ess ica al', [1, 3]), ('ess ica al ba', [1, 3]), ('scar lett j oh', [3, 1]), ('lett j oh ansson', [3, 1]), ('bob back lund vs', [0, 4]), ('b ret h art', [0, 4]), ('w w f world', [0, 4]), ('ese w ither sp', [4, 0]), ('ke vin k line', [4, 0]), ('sp encer tr acy', [4, 0]), ('j ur assic park', [4, 0]), ('512 mb v iking', [0, 4]), ('gig ab eat room', [0, 4]), ('km 25 g 0', [4, 0]), ('co le man silk', [4, 0]), ('cir que du sole', [4, 0]), ('que du sole il', [4, 0]), ('knights tem pl ar', [0, 3]), ('marg aret star bird', [0, 3]), ('mag dal ene conspiracy', [0, 3]), ('h ink ley residents', [0, 3]), ('j apan ese high', [0, 3]), ('apan ese high school', [0, 3]), ('j apan ese students', [0, 3]), ('j im hell wig', [0, 3]), ('ch ay ef sky', [1, 2]), ('john ny de pp', [1, 2]), ('lex andra rip ley', [0, 3]), ('g ary fl eder', [0, 3]), ('un sy mp athetic', [1, 2]), ('christ ian b ale', [1, 2]), ('mar lon way ans', [1, 2]), ('j enn ifer conn', [1, 2]), ('enn ifer conn elly', [1, 2]), ('ell en burst yn', [1, 2]), ('ow en h art', [0, 3]), ('w f world champion', [0, 3]), ('love l u cy', [0, 3]), ('ge orge clo oney', [3, 0]), ('high school reunion collection', [3, 0]), ('j ul ie new', [3, 0]), ('ul ie new mar', [3, 0]), ('ra quel wel ch', [2, 1]), ('mb v iking card', [0, 3]), ('har ry pot ter', [3, 0]), ('w ap 54 g', [3, 0]), ('16 80 x 10', [3, 0]), ('80 x 10 50', [3, 0]), ('rec ond ition ed', [1, 2]), ('ber ks hire hath', [3, 0]), ('ks hire hath away', [3, 0]), ('n ich olas sparks', [1, 2]), ('ave rob iche aux', [0, 3]), ('ann ie wil kes', [3, 0]), ('k itty f oyle', [2, 0]), ('j ames cra ig', [2, 0]), ('k ath arine hep', [1, 1]), ('ath arine hep burn', [1, 1]), ('rich ard g ere', [0, 2]), ('al bert fin ney', [0, 2]), ('ese high school students', [0, 2]), ('r yan st arr', [0, 2]), ('ary lam bert tries', [0, 2]), ('looks like war ners', [0, 2]), ('like war ners gave', [0, 2]), ('gets better film roles', [0, 2]), ('audio enc od ings', [0, 2]), ('p addy ch ay', [1, 1]), ('addy ch ay ef', [1, 1]), ('je ff gold bl', [1, 1]), ('ff gold bl um', [1, 1]), ('miss iss ippi burning', [0, 2]), ('sand ra bull ock', [0, 2]), ('scattered h odge p', [0, 2]), ('h odge p odge', [0, 2]), ('odge p odge statements', [0, 2]), ('given cl ic hed', [0, 2]), ('cl ic hed dialogue', [0, 2]), ('miss congen ial ity', [0, 2]), ('pat rick mc go', [0, 2]), ('rick mc go ohan', [0, 2]), ('equ ating elong ated', [0, 2]), ('ating elong ated facial', [0, 2]), ('elong ated facial shots', [0, 2]), ('making ser gio le', [0, 2]), ('ser gio le one', [0, 2]), ('gio le one western', [0, 2])]\n"]}],"source":["import glob\n","\n","# all your saved model prediction results from 1.3 Fine-Grained Validation\n","prediction_files = glob.glob(os.path.join('predictions/', 'test_*.jsonl'))\n","\n","# TODO: Define your tokenizer\n","checkpoint_path = 'models/lr1e-05-warmup0.3'\n","tokenizer = RobertaTokenizer.from_pretrained(checkpoint_path)\n","ngrams = ngram_extraction(prediction_files, tokenizer)\n","\n","top_100 = {}\n","for n, counts in enumerate(ngrams):\n","    # TODO: find top-100 n-grams (n=1,2,3 or 4) associated with the greatest frequency of appearance\n","    sorted_ngrams = sorted(counts.items(), key=lambda item: sum(item[1]), reverse=True)\n","\n","    top_100_freq = sorted_ngrams[:100]\n","\n","    print(f'Top-100 most frequent {n+1}-grams:')\n","    print(top_100_freq)\n","\n","    top_100[n] = top_100_freq"]},{"cell_type":"markdown","metadata":{"id":"pz25EvuI_3vU"},"source":["**Among each type of top-100 frequent n-grams above**, we aim to further find out the n-grams which **most likely** lead to *positive*/*negative* predictions (positive/negative shortcut features).\n","\n","TODOðŸ”»: (3) [coding&text question] Design **two different methods to re-rank** the top-100 n-grams to extract shortcut features. For each method, you should extract **1** feature in each of n-grams group (n=1, 2, 3, 4) for positve and negative prediction (1\\*4\\*2=8 features in total for 1 method).\n","\n","Explain each of your design choices in natural language, and compare which method finds more reasonable patterns.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yq2cVOaWTEYw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711566988816,"user_tz":-60,"elapsed":10,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"70fab32f-30f7-48b9-e90a-8fbe128d1df8"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Top positive shortcut features for each n-gram group:\n","1-gram: ('love', [148, 61])\n","2-gram: ('john son', [19, 0])\n","3-gram: ('g iov anni', [8, 0])\n","4-gram: ('w ither sp oon', [6, 0])\n","\n","Top negative shortcut features for each n-gram group:\n","1-gram: ('bad', [34, 130])\n","2-gram: ('da v', [0, 14])\n","3-gram: ('gig ab eat', [0, 13])\n","4-gram: ('j apan ese culture', [0, 5])\n"]}],"source":["# TODO: [Method 1] find top-1 positive and negative patterns\n","def extract_shortcut_features_ratio(ngrams_list):\n","    shortcut_features = {'positive': [], 'negative': []}\n","\n","    for ngrams in ngrams_list:\n","\n","        sorted_pos = sorted(ngrams, key=lambda x: (x[1][0] / (x[1][1] + 1)), reverse=True)\n","        sorted_neg = sorted(ngrams, key=lambda x: (x[1][1] / (x[1][0] + 1)), reverse=True)\n","\n","        shortcut_features['positive'].append(sorted_pos[0])\n","        shortcut_features['negative'].append(sorted_neg[0])\n","\n","    return shortcut_features\n","\n","top_100_lists = [top_100[n] for n in range(4)]\n","\n","shortcut_features = extract_shortcut_features_ratio(top_100_lists)\n","\n","for type in ['positive', 'negative']:\n","    print(f\"\\nTop {type} shortcut features for each n-gram group:\")\n","    for i, feature in enumerate(shortcut_features[type]):\n","        print(f\"{i+1}-gram: {feature}\")"]},{"cell_type":"markdown","source":["**Explanation of Method 1 :**\n","\n","**Ratio of Positive to Negative Predictions**\n","\n","\n","\n","  * Explanation: This method calculates the ratio of positive to negative predictions for each n-gram. The intuition is that n-grams with a high positive-to-negative ratio are strong indicators of positive sentiment, while those with a low ratio indicate negative sentiment. This method reflects the n-gram's tendency to appear in positive or negative contexts.\n","\n","  * Calculation: For each n-gram, calculate the ratio of positive to negative predictions (pos/neg).\n","  For positive shortcut features, select the n-gram with the highest pos/neg ratio in each n-gram group.\n","  Inversly for negative shortcut features."],"metadata":{"id":"vd4U360facff"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMJXp1flfmao","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711567877841,"user_tz":-60,"elapsed":221,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"7e45c4b3-6320-4890-e1c8-cf9964e3616d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Top positive shortcut features for each n-gram group using PSS:\n","1-gram: bad\n","2-gram: highly recommend\n","3-gram: gig ab eat\n","4-gram: w ither sp oon\n","\n","Top negative shortcut features for each n-gram group using PSS:\n","1-gram: able\n","2-gram: b rit\n","3-gram: eng ross ing\n","4-gram: ff gold bl um\n"]}],"source":["# TODO: [Method 2] find top-1 positive and negative patterns\n","\n","# Used ChatGPT\n","# Prompt used: can you give me another method similar to the ratio of positive to negative predictions for identifying top-1 positive and negative shortcut features\n","# Output of the tool:\n","# For a second method to identify top-1 positive and negative shortcut features, focusing on the Predictive Strength Score (PSS), let's define and apply it as follows:\n","# Method 2: Predictive Strength Score (PSS) for Shortcut Features\n","# The Predictive Strength Score (PSS) is a method that quantifies the indicative power of an n-gram for a particular sentiment (positive or negative) by considering both the frequency of its occurrence in the desired sentiment class and its frequency in the opposite class. The score aims to balance raw frequency with the contrast between sentiment classes, offering a nuanced view of an n-gram's predictive value.\n","# Formula for PSS\n","# Given \\(pos\\) as the count of an n-gram in positive samples and \\(neg\\) as its count in negative samples, the PSS can be calculated as follows:\n","# \\[ PSS = \\left( \\frac{|pos - neg|}{pos + neg} \\right) \\times \\log(pos + neg + 1) \\]\n","# This formula incorporates both the difference in sentiment counts (providing contrast) and the overall frequency of the n-gram (adding a weighting factor that prioritizes more common n-grams).\n","\n","from math import log\n","\n","def predictive_strength_score(pos, neg):\n","    return (abs(pos - neg) / (pos + neg)) * log(pos + neg + 1)\n","\n","def extract_shortcut_features_pss(ngrams_list):\n","    shortcut_features = {'positive': [], 'negative': []}\n","\n","    for ngrams in ngrams_list:\n","        ngrams_with_pss = []\n","\n","        for ngram, counts in ngrams:\n","            pos, neg = counts\n","            pss = predictive_strength_score(pos, neg)\n","            ngrams_with_pss.append((ngram, pss))\n","\n","        ngrams_sorted = sorted(ngrams_with_pss, key=lambda x: x[1], reverse=True)\n","\n","        if ngrams_sorted:\n","            shortcut_features['positive'].append(ngrams_sorted[0][0])\n","            shortcut_features['negative'].append(ngrams_sorted[-1][0])\n","    return shortcut_features\n","\n","shortcut_features_pss = extract_shortcut_features_pss(top_100_lists)\n","\n","for sentiment in ['positive', 'negative']:\n","    print(f\"\\nTop {sentiment} shortcut features for each n-gram group using PSS:\")\n","    for i, feature in enumerate(shortcut_features_pss[sentiment]):\n","        print(f\"{i+1}-gram: {feature}\")\n"]},{"cell_type":"markdown","source":["**Explanation of Method 2:**\n","**Predictive Strength Score (PSS)**\n","\n","  - Explanation: This method computes a Predictive Strength Score (PSS) for each n-gram, which combines both the frequency of the n-gram and its distribution across positive and negative predictions. The score is designed to prioritize n-grams not only frequently appearing but also showing a clear preference for positive or negative predictions.\n","\n","  - Calculation: Compute the score as follows: PSS = (abs(pos - neg) / (pos + neg)) * log(pos + neg).\n","  For positive shortcut features, select the n-gram with the highest PSS where pos > neg in each n-gram group.\n","  Inversly for negative shortcut features."],"metadata":{"id":"ebOcgG40azhN"}},{"cell_type":"markdown","metadata":{"id":"xvKyF0UFuXXM"},"source":["TODOðŸ”»: Compare and discuss the results from two methods above."]},{"cell_type":"markdown","source":["\n","* Method 1 : (Pos/Neg Ratio) straightforwardly identifies n-grams that lean strongly towards positive or negative sentiments based on their occurrence ratio. It is directly linked to the sentiment indication of the n-gram. However, it might favor n-grams that are very rare but not necessarily indicative of the general sentiment.\n","\n","* Method 2 (PSS) : tries to balance the predictiveness of an n-gram with its occurrence frequency, making sure that selected features are both indicative of sentiment and sufficiently common.\n","\n","* Both methods found the shortcut \"bad\" for 1-gram, however, method 1 found it as the top negative shortcut feature while method 2 found it as the top positive shortcut feature. Even if the word bad has a negative connotation in general, sometimes it can be used as not bad which is not a negative connotation.\n","\n"],"metadata":{"id":"WoPuXwj3bFgz"}},{"cell_type":"markdown","metadata":{"id":"wjVti-vL_3vV"},"source":["## ðŸŽ¯ Q2.3: **Case Study (8 pts)**\n","\n","TODOðŸ”»: Among the shortcut features you found in 2.1, find out **4 representative** cases (pair of `\\[review, n-gram feature\\]`) where the shortcut feature **will lead to a wrong prediction**.\n","\n","For example, the 1-gram feature \"excellent\" has been considered as a shortcut for *positive* sentiment, while the ground-truth label of the given review containing \"excellent\" is *negative*.\n","\n","**Questions:**\n","- Based on your case study, do you detect any limitations of the n-gram patterns?\n","- Which type of n-gram (1/2/3/4-gram) pattern is more robust to be used for sentiment prediction shortcut and why?"]},{"cell_type":"code","source":["data_dir = 'data'\n","data_train_path = os.path.join(data_dir, 'train_sa.jsonl')\n","\n","# Load the dataset into a list of dictionaries\n","dataset = []\n","with jsonlines.open(data_train_path, \"r\") as reader:\n","    for sample in reader:\n","        dataset.append(sample)"],"metadata":{"id":"GvEjmo-qcLpS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XyPmb01_3vV"},"outputs":[],"source":["# Used ChatGPT\n","# Prompt used: \"Write a Python function to find cases in a dataset where the presence of an n-gram contradicts the expected label.\"\n","# Output of the tool: The function below\n","# To verify the correctness of the output, I tested the function for the 4 representative cases where the shortcut feature will lead to a wrong prediction.\n","# Modifications made: None required.\n","# Explanation of AI-based code functionality:\n","# It checks if the specified n-gram is present in each review's text. If the n-gram is found, the function then checks if the review's label\n","# contradicts the expected label associated with that n-gram\n","\n","def find_misleading_cases(dataset, ngram, expected_label):\n","    \"\"\"\n","    Find cases where the presence of an n-gram contradicts the expected label.\n","\n","    :param dataset: List of dictionaries, each representing a review.\n","    :param ngram: The n-gram string to search for.\n","    :param expected_label: The label ('positive' or 'negative') the n-gram is usually associated with.\n","    :return: A list of reviews where the n-gram is present but the label contradicts the expected sentiment.\n","    \"\"\"\n","    misleading_cases = []\n","    for review in dataset:\n","        # Check if the n-gram is in the review text\n","        if ngram in review['review'].lower():\n","            # Check if the review label contradicts the expected label associated with the n-gram\n","            actual_label = review['label']\n","            if (expected_label == 'positive' and actual_label != 'positive') or (expected_label == 'negative' and actual_label != 'negative'):\n","                misleading_cases.append(review)\n","    return misleading_cases"]},{"cell_type":"code","source":["ngram = \"excellent\"\n","expected_label = \"positive\"\n","misleading_cases = find_misleading_cases(dataset, ngram, expected_label)\n","\n","for case in misleading_cases[:4]:\n","    print(f\"Review: {case['review']}\\nLabel: {case['label']}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u4krIKiqdnA8","executionInfo":{"status":"ok","timestamp":1711569010263,"user_tz":-60,"elapsed":225,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"7db125b8-04a5-4da6-f01b-0db940a53d7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Review: There are plenty of excellent Play Therapy books out there, this is not one of them.  Maybe it's just me, but a play therapy book should be way more user friendly.  I need quick access, color and... ART\n","Label: negative\n","\n","Review: I'm an avid reader of history, as well as processing a degree in the subject.  So imagine my surprise when, after receiving this book from a friend of mine for Christmas, I read the erroneous account of the Children's Crusade of 1212.  I had done research on this topic, so I was horrified to read the completely inaccurate account of what occurred.  Had the author not read any historical analysis on the subject from the last 50 years?  If he had, he would have realized that there were actually two crusades - one consisting of mainly French people led by Stephen of Cloyes who, when told to turn back by King Philip II, did so.  That ended that crusade.  The other one, led by a shepherd from Germany named Nicholas, led a group across the Alps into Italy.  Some left for home while others continued on to Rome.  It's interesting to note that in Rome, many received dispensations from their crusading vows because these \"children\" were either too old or because they were pregnant.  Perhaps until relatively recently, people believed in the Children's Crusade because it represented a morality play or because some historians gave too much credit to chronicles (like Chronica Albrici monachi Trium Fontium) which were written long after the crusade supposedly occurred, rather than relying on more contemporary sources.  Nor did they realize that the latin word \"pueri\" used in the chronicles can have several meanings (such as unmarried men rather than  children). In the final analysis, you just can't rely on books like these to really teach you history.  The best you can do is read what they tell you and then try to verify it.  If only the author had bothered to check the History Channel's own account of the Children's Crusade on their website, or perhaps read the excellent paper done by Peter Raedts in the Journal of Medieval History, or even just checked out the brief but accurate entry online in wikipedia.  The book gave two pages to this event, and sadly got it completely wrong. For this gross oversight, I am compelled to give it one star.  Readers of history, never just believe what you read - verify\n","Label: negative\n","\n","Review: A very disappointing book, and again, makes me very wary of any item on a \"New York Times Best Seller List\" or any other list, and really any big name reviewer recommendation. This book has one 50 page chapter which is excellent - the recap of the Thompson \"Shot heard around the world\" homerun. If you are a baseball fan as I am, that (barely) made the book worthwhile. The other 700 pages are disjoint, incomprehensible, and really trivial. You come away thinking \"I really don't care bout this stuff\" - I don't care about the dynamics of trash, about J Edgar Hoover's bathroom habits, Klara Sax's artistic depression and annoying infatuations about roof tops, about characters that just don't have much depth to really care about. At the end, even potentially interesting characters like Albert (the one on the baseball quest) become hard to bear. The whole book in fact had the feel of going to a senior citizen home. I read this book, as perhaps others did, expecting a book about the mafia or about other figures in the underworld. What I got was a book with a good first chapter, some interesting tidbits here and there about how life used to be in the 50's (fear of nuclear weapons, duck and cover classroom exercies), and a lot of knowledge about the trash business. The \"masterful\" epilogue, as other reviewers call it, really does no justice to a reader who has spent so much time toiling over Delillo's ragtag story telling. I am ok with stream of consciousness writing, or nonchronological chapters, or even random chapters, but the methods used in thsi book served no purpose other than to further make the reading difficult. Was this book about how mundane our lives are versus the bigger things that occur around us? Or how each of us is on an individual quest? Or that the 50's-60's-70's-80's were just years of fear, and the 90's are years of greed? I am still unsure, I still don't think any of these themes came across well. Please - not pretending to be a sophisticate, intellectual, or professional critic - but this book is really not worth the time or effort. You can be staisfied by a number of other books, including the \"The Corrections\" by Franzen or \"The Cold Six Thousand\" by Ellroy  which cover a similar period or similar themes and do not make you so frustrated or disappointed at the end. Read the first chapter, then put the book down or you will be sorely disappointed and have wasted an awful lot of time.\n","Label: negative\n","\n","Review: I was very excited about this film as a fan of both the book and Ghibli Studios previous works. All the greater the disappointment then, when less than half way through, the story becomes something utterly unrecognizable from the original, several key characters are lumped together most confusingly, and others lose all their complex charm. What a shame. If you have never read the book and are a fan of Miyazaki, by all means buy this. You're very likely to enjoy the excellent animation and yet another unusual story from him. If you are a fan of the book, steer well clear. You're likely to want to poke Miyazaki with a stick and see if his head turns into a turnip or his turnip into a head.\n","Label: negative\n","\n"]}]},{"cell_type":"code","source":["ngram = \"love\"\n","expected_label = \"positive\"\n","misleading_cases = find_misleading_cases(dataset, ngram, expected_label)\n","\n","for case in misleading_cases[:4]:\n","    print(f\"Review: {case['review']}\\nLabel: {case['label']}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dez_duLrdrEb","executionInfo":{"status":"ok","timestamp":1711569036430,"user_tz":-60,"elapsed":229,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"aa38632a-3665-4488-e36e-24762a5ffef1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Review: Very average book!  Frei doesn't go down as one of my favorite sports writers. Loves to sensationlize rather then just tell a great story\n","Label: negative\n","\n","Review: Just finished reading this novel this morning and I think it is probably the weakest one of the lot.  Like a lot of reviewers, I didn't find this one anywhere near as gripping or well-developed plotwise as a number of the earlier ones in the Stephanie Plum series.  As well, the supposed sexual tension- love triangle between Stephanie, Joe and Ranger is approaching its use-by date.  It is definitely becoming boring and I think Ms Evanovich needs to resolve Stephanie's romantic situation once and for all.  I still enjoy characters like Lula and Grandma Mazur but a lot of the situations Stephanie found herself in fell flat with me.  There are only so many times she can have her car blown up, handbag destroyed, apartment broken into, etc, before it becomes tiresome and repetitive.  I still think there is a spark of life in the Stephanie Plum series, but I hope the next novel is a better effort and not so much by-the-numbers as this one\n","Label: negative\n","\n","Review: I gave this book two stars because the book was a little boring and dragged on in some areas. The best information was in the margins and the quotes not really any of the information in the book. I have read: Carolyn, Bill, and Amy's books and I want to give this one a try. This book is truly for The Apprentice TV lover but if you are looking for business advice this is not the book for you\n","Label: negative\n","\n","Review: \"The Other Boleyn Girl\" and \"The Queen's Fool\" are two of the best books I've ever read--and I've read a LOT of books, especially about this period, which especially fascinates me. They were gripping to the end, and my involvement with the characters was complete--true masterpieces. So whenever a new book by Philippa Gregory comes out, I buy it, hoping to have the same experience, and lately have been sorely disappointed. \"The Virgin's Lover\" fell flat, and now I am trying to read this book but not finding myself engaged, must give it up. One reviewer suggested the books were being written too quickly--that is always an easy guess, however it depends on the writer. Anthony Trollope, for instance, was able to turn out amazing book after amazing book with incredible speed. That may or may not be the case with Gregory. I know, as a writer myself, that there are times when the work flows out, feeling almost as if it's channeled from another source, and times when it's harder, more mechanical. The more challenging situations are those when I'm not entirely convinced or in love with my subject. So to me, neither of these books feels as if they spring from a natural impulse. They feel forced, constructed. I'm going back to Trollope\n","Label: negative\n","\n"]}]},{"cell_type":"code","source":["ngram = \"much better\"\n","expected_label = \"positive\"\n","misleading_cases = find_misleading_cases(dataset, ngram, expected_label)\n","\n","for case in misleading_cases[:4]:\n","    print(f\"Review: {case['review']}\\nLabel: {case['label']}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iKJI-aG-dqPr","executionInfo":{"status":"ok","timestamp":1711569228038,"user_tz":-60,"elapsed":242,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"0d709f82-2b8d-487e-bc2d-80cab102012a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Review: Didn't care for this book at all.  If you want to learn about Stephen King you'd be much better off reading or listening to his book &quot;On Writing.&quot\n","Label: negative\n","\n","Review: ...are incorrect and that his fractal models are much better. Of course if his models were worth more than the paper they're written on Mandelbrot wouldn't have to write books like this because he'd be cleaning out everyone else's wallets on the stock market. In particular, if it's true that extreme events are more unlikely than most people think then he could easily exploit this with a suitable derivative. But the fact is, Mandelbrot doesn't know anything that countless other traders don't already know. So instead Mandelbrot is forced to resort to telling people how smart he is through his books rather than actually being smart enough to make a killing on Wall Street. But I am glad I read this book. Having seen Mandelbrot 'on tour' a few years ago I developed a strong prejudice against him. But reading this book has convinced me that my prejudice is entirely justified. He can never just state a fact. Instead he always has a to phrase it as \"my work shows that...\" or \"I demonstrated that...\" even if he makes the same claim again and again. If he can try to take credit for other people's work he'll do so. He even managed to find someone to write an introduction for the book who was prepared to refer to the Levy distribution as the Mandelbrot-Levy distribution. The most egregious example of self-aggrandisement has to be the caption to a picture of the Brooks-Metelsky-Mandelbrot set where he mentions that two mathematicians only scaled part way up his 'Everest' of this set and received Fields medals for this work - the implication of course being that Mandelbrot has actually seen the view from the summit. Disgustingly he doesn't even deign to mention the names of these mathematicians. One time I criticised Mandelbrot publicly and someone responded by pointing out how many peer reviewed papers he had published so he can't be all bad. But in this book Mandelbrot actually reveals to us how he used his social network to work around the peer review system. Astonishing! I think it's also worth pointing out that there are some interesting ideas in this book and so it's not completely valueless. But I found the discussion of multifractal time methods (which make up a large part of the book) to generate plausible looking but fake price charts to be a bit pointless. I work in computer graphics and use similar methods to generate random surface detail all the time, but it doesn't mean I have a deep understanding of the statistics of random surfaces. It just means I know how to fool the eye\n","Label: negative\n","\n","Review: I've been a fan of Heinlein's SF for just about as long as I've been an SF fan at all (somewhere around thirty-five years). But when I read this collection some twenty-odd years ago, it nearly turned me off to the old man altogether. The fiction is pretty good (although even that isn't Heinlein's best). But to describe the nonfiction accurately, I'd have to use words that Amazon will remove from the review anyway. For the most part, the pieces collected here represent a side of Heinlein I strongly dislike. Though I respect _Starship Troopers_, it's never going to be my favorite Heinlein novel no matter how many times we quibble over the precise definition of \"fascism\" -- and I'm not going to have much respect for the nonfiction in this collection. And mind you, it's not because I disagree with Heinlein's opinions; it's because he gave bad arguments for them and called other people nasty names when they differed. Some of the stuff collected here is tendentious to the point of being propagandistic. (At least in his fiction, he was able to create characters with points of view that differed from his and get into their heads well enough to present them sympathetically -- in short, to grok them. Not here, boy.) Heinlein (who bought into the Korzybski/General Semantics fad pretty early on) spent a lot of years dismissing philosophers as tailchasers who derive their premises from their conclusions. But his own attempt at philosophy, as represented here in e.g. \"The Pragmatics of Patriotism\", is very nearly the worst writing on ethical philosophy I've ever seen. Then, too, people who knew Heinlein report that despite his overall gentlemanly demeanor, he could be pretty churlish toward people who disagreed with him. Well, he's certainly unpleasant here; anybody who doesn't agree with him on the need for massive nuclear buildup is dismissed as a poltroon or a custard-head. Even in the unlikely event that I thought he were _right_, I wouldn't find this a very helpful approach. Perhaps more surprisingly, his popular writings on _science_ aren't very good. Asimov's reputation as the \"great explainer\" is in no danger here. This volume is second only to _Grumbles from the Grave_ in cementing Heinlein's posthumous reputation as a rather mean-spirited fellow whose fictional characters were generally much better company than he was. When I want Heinleinian company, I'll stick to D.B. Davis, Manuel Garcia O'Kelly Davis, and (maybe) Lazarus Long. And when I want to read some humane nonfiction by an SF master, I'll still turn to Asimov. I credit Heinlein with three magisterial novels, several imperfect-but-great ones, and a good number of brilliant short stories. But the stuff in this book should have stayed in his drawer\n","Label: negative\n","\n","Review: I am a huge fan of Kathy Reichs' novels and of the TV Series, Bones. However, this book was quite a disappointment to me. I did not care for the way the author wrote dialogue -- Brennan's dialog is not that simplimatic on the TV program, and she certainly would never say, \"Where at!?\" The author way overused the joke where Brennan did not \"know what that means.\" All I could think of was how much better this novel would have been if written by Reichs herself. I started to jot down the holes in the story but quit after a while\n","Label: negative\n","\n"]}]},{"cell_type":"code","source":["ngram = \"like\"\n","expected_label = \"positive\"\n","misleading_cases = find_misleading_cases(dataset, ngram, expected_label)\n","\n","for case in misleading_cases[:4]:\n","    print(f\"Review: {case['review']}\\nLabel: {case['label']}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lq_w5MQ3d5PQ","executionInfo":{"status":"ok","timestamp":1711569124524,"user_tz":-60,"elapsed":216,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"1c913847-79b0-40c0-ef40-e170b2e5165f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Review: I like to use the Amazon reviews when purchasing books, especially alert for dissenting perceptions about higly rated items, which usually disuades me from a selection.  So I offer this review that seriously questions the popularity of this work - I found it smug, self-serving and self-indulgent, written by a person with little or no empathy, especially for the people he castigates. For example, his portrayal of the family therapist seems implausible and reaches for effect and panders to the \"shrink\" bashers of the world. This \"play for effect\" tone throughout the book was very distasteful to me\n","Label: negative\n","\n","Review: I picked up the first book in this series (The Eyre Affair) based purely on its premise and was left somewhat underwhelmed. Still, the potential for the series seemed so large that I went ahead and read this second one too, only to be even less enchanted with the franchise. This is a pure sequel, and any newcomers are advised to read the misadventures of Thursday Next is strict order, lest one miss out of allusions to past events. Although... on further consideration, maybe it doesn't matter, since clearly anything can and will happen in this series, and Fforde isn't all that interested in keeping to a linear plotline anyway. The setting is the same as the first book, an alternate mid-1980s England in which literature is the preeminent social preoccupation. Fresh off the events of \"The Eyre Affair\", Thursday Next (a police officer specializing in literature related crimes, such as first-edition forgeries, valuable manuscript thefts, and the like) is gritting her teeth through a new round of fame as the woman who saved Jane Eyre (and changed the ending for the better), when all she wants to do is cuddle up at home with her new husband Landen. Unfortunately, the evil Goliath Corporation has managed to use a corrupt member of the Chronogaurd (timestream police) to delete Landen from this timeline and are holding his existence hostage. In the first book Thursday imprisoned one of their top men inside Poe's \"The Raven\", and it seems they want him back. This a potentially interesting plot, but it keeps get lost amidst all the other things Fforde throws into the mix. Most notable are a series of strange coincidences which keep coming close to killing Thursday (and are also linked to events in the first book). Another plotline concerns the discovery of a \"lost\" Shakespeare play, which looks to be the most important literary event of the century, if Thursday can authenticate it. There's also the small matter of Thursday's pregnancy. And just when one is comfortable with Thursday's role as a \"SpecOps Litratech\", and that whole milieu, she's thrown into an entirely new one as a member of \"Jurisfiction\", a kind of police comprised of book characters who move around in different literary works and maintain order... Finally, her father pops up to inform her that something in the timestream has gone wrong and the entire world is going to be turned into a mass of pink sludge in a few days unless he can figure it out, and can she help him. Phew! I've probably missed one or two elements, but you get the idea. Fforde is just brimming with nifty ideas, but the shame of it is that he can't stop and give any of them the attention they deserve. It's impossible to get invested in any of the plotlines when you know he's just going to move on to something else in a few pages, and it's impossible to care about the characters when their existence is utterly malleable, as is time and place. I suppose it's all meant to be puckish good fun, but the overall effect is more an attention deficit disorder Nancy Drew heroine meets a poor-man's Douglas Adams. The book has its occasional moments, but the humor is far too broad and unsubtle, and there's absolutely no narrative tension. All the literary in jokes in the world can't save this shambling wreck, and I don't think I'll be moving on to the next book.\n","Label: negative\n","\n","Review: This is the sixth book in the Left Behind series. It is also the first one in the series I did not finish in one sitting. It did not flow as well as the others, so as to lend itself to a complete reading in one evening. The book also spends a lot of time re-telling what we read in previous books. I felt this time like I read half a novel. The book starts in the thirty-eighth month into the tribulation. But most of the characters seem a little too self serving in this book. Allowing their own desires to guide their actions. Whether that desire be derived from rage, revenge, self pity, or even guilt. Rayford spends most of his time focusing on Hattie or his desire to kill the Anti-Christ. Though I do like the description of the handgun that Rayford procures. And when the time comes for the assassination, there are a few characters to chose from. The book ends in an old time cliff hanger where you are left guessing who it is that assassinated the Anti-Christ. Stay tuned for the next volume. I have read this far, so I will finish the series. Though most people my find this book suspensful, I did not\n","Label: negative\n","\n","Review: Oh, yes - this is a very fine book... for me to poop on! In other words, it's a JOKE book. It's just another one of a half dozen duds out there, all trying to slam whoever happens to be the #1 retailer at the time - this decade it's Wal-Mart. Foreign countries paying their workers less? Blame Wal-Mart! Worker insurance prices going sky high because the medical industry over-estimates it's fees? Blame Wal-Mart! Crooked unions feeling threatened they'll get their butts kicked by non-union competition? Blame Wal-Mart! Etc., etc. You just have to laugh at how these semi-journalistic schlockmeisters try so desperately to compare themselves to a real activist like Micheal Moore, who actually accomplishes some social good once in awhile. But you know who's REALLY laughing? That's right - Wal-Mart. That's because they've ALREADY built their thousands of stores, and been in business for over 40 years, so it's way too late now for these mealy-mouthed media wannabes to do anything about it! And what do these ersatz authors expect at this very late date, anyway? That millions of consumers, during these roller-coaster economic times, will be instantly brainwashed by these new crackpot books and then suddenly start to \"boycott lower prices\"? Ha! Ha! Ha! Like I said, this has got to be just another joke book\n","Label: negative\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"c45cvlUqufRR"},"source":["TODOðŸ”»: (Write your case study discussions and answers to the questions here.)"]},{"cell_type":"markdown","source":["Limitations of n-gram patterns for sentiment prediction:\n","\n","1. **Context Sensitivity:** N-gram patterns lack contextual understanding, leading to misinterpretations of sentiment, especially in cases of sarcasm, irony, or nuanced language.\n","2. **Over-Reliance on Specific Words:** N-grams focus solely on the presence or absence of specific words or phrases, ignoring the broader context of the text.\n","3. **Limited Generalization:** Higher-order n-grams may capture more complex patterns but risk overfitting to specific language patterns in the training data, reducing generalization to unseen data.\n","4. **Neglect of Negations:** N-grams may fail to account for negations (e.g., \"not good\"), resulting in incorrect sentiment classification.\n","\n","Regarding the robustness of n-gram patterns for sentiment prediction shortcuts, the choice depends on the balance between specificity and generalization:\n","\n","- **1-gram (Unigram):** Offers simplicity and generalization but may lack context and nuanced understanding, leading to misclassifications.\n","- **2-gram (Bigram):** Captures slightly more context than unigrams but still suffers from limited contextual understanding and may not adequately handle longer phrases.\n","- **3-gram (Trigram) and 4-gram (Four-gram):** These higher-order n-grams capture more context and can potentially discern more nuanced sentiment patterns. However, they also increase the risk of overfitting to specific language patterns in the training data, reducing generalization to unseen data.\n","\n","Higher-order n-grams (3-gram and 4-gram) may offer more context and potentially better sentiment prediction, they are also more prone to overfitting, which is the case in this context. Thus, I recommend using 1-gram (unigram) or 2-gram (bigram) for sentiment prediction."],"metadata":{"id":"UQs9l9Q5djfx"}},{"cell_type":"markdown","metadata":{"id":"yND0DEfT-eXn"},"source":["<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n","\n","# Part 3: Annotate New Data (25 pts)\n","\n","In this part, you will **annotate** the gold labels of some **new** SA data samples, and measure the degree of **agreement** between your and **one or two partners'** annotations.\n","    \n","</div>"]},{"cell_type":"markdown","metadata":{"id":"ZQNXrRHr_3vV"},"source":["## ðŸŽ¯ Q3.1: **Write an Annotation Guideline (5 pts)**\n","\n","TODOðŸ”»: Imagine that you are going to assign this annotation task to a crowdsourcing worker, who is completely not familiar with computer science and NLP. Think about how you are going to explain this annotation task to him in order to guide him do a decent job. Write an annotation guideline for such a worker who are going to do this task for you.\n","\n","**Note:** You should come up with your own guideline without the help of your partner(s) in later Part 3.2"]},{"cell_type":"markdown","source":["**Guidelines for Sentiment Annotation:**\n","\n","---\n","\n","1. **Read Carefully:**\n","   - Take your time to read each text completely. Understanding the overall context is crucial for accurately determining the sentiment.\n","\n","2. **Consider Overall Sentiment:**\n","   - Focus on the overall sentiment of the entire text rather than isolated words. Some texts may contain both positive and negative words, but what matters is the general feeling conveyed. For example, a reviewer might criticize some aspects of the product but still recommend it, this is considered a positive review. Another example, if a reviewer considers the product to not be worth the money even if it works and suggests another product instead, it is considered a negative review.\n","\n","3. **Look for Keywords:**\n","   - Certain words or phrases can be strong indicators of sentiment. For example, words like \"love\", \"happy\", \"great\" suggest positive sentiments, whereas \"hate\", \"disappointed\", \"worst\" suggest negative sentiments. However, be mindful of the context, as the same word can be used sarcastically or in different contexts.\n","\n","4. **Beware of Sarcasm and Irony:**\n","   - Sometimes, a text may seem positive at first glance but is actually negative due to sarcasm. Pay attention to clues that might indicate the author is being sarcastic or ironic.\n","\n","5. **Consider All Aspects:**\n","   - Sometimes, the criticism may not be about the product itself, but rather about specific aspects such as the storyline or historical accuracy. Take into account the broader context of the review when assessing sentiment.\n","\n","6. **Clarify Confusion:**\n","   - If the criticism pertains to elements beyond the product, such as historical accuracy or storytelling, ensure to distinguish between criticism of the product itself and criticism of external factors."],"metadata":{"id":"7niRkxH0e7nT"}},{"cell_type":"markdown","metadata":{"id":"XBWK4Bw__3vV"},"source":["## ðŸŽ¯ Q3.2: **Annotate Your Datapoints with Partner(s) (8 pts)**\n","\n","TODOðŸ”»: Annotate 80 datapoints (20 in each domain of \"books\", \"dvd\", \"electronics\" and \"housewares\") assigned to you and your partner(s), by editing the value of the key **\"label\"** in each datapoint. You and your partner(s) should annotate **independently of each other**, i.e., each of you provide your own 80 annotations.\n","\n","Please find your assigned annotation dataset **ID** and **your partner(s)** according to this [list](https://docs.google.com/spreadsheets/d/1hOwBUb8XE8fitYa4hlAwq8mARZe3ZsL4/edit?usp=sharing&ouid=108194779329215429936&rtpof=true&sd=true). Your annotation dataset can be found [here](https://drive.google.com/drive/folders/1IHXU_v3PDGbZG6r9T5LdjKJkHQ351Mb4?usp=sharing)."]},{"cell_type":"markdown","metadata":{"id":"IWhjTn2fQ5YE"},"source":["**Name your annotated file as `<your_assigned_dataset_id>-<your_sciper_number>.jsonl`.**\n","\n","**You should also submit your partner's annotated file `<assigned_dataset_id>-<your_partner_sciper_number>.jsonl`.**"]},{"cell_type":"code","source":["# Token from Ed-discussion : Script for fast annotation #357\n","# Modified the code to add a function to get current progress by counting the number of entries in the annotations file\n","# Indeed, my colab was crashing multiple times in the middle of annotating.\n","# Prompt used: \"How to count the number of items in a JSONLines file in Python.\"\n","# Output of the tool: The function get_current_progress\n","# To verify the correctness of the output, I just checked that indeed when colab was crashing it started from the last review I annotated\n","\n","from IPython.display import clear_output\n","\n","dataset_id = 3\n","scipher = SCIPER\n","\n","annotations_file_path = f\"data/{dataset_id}-{scipher}.jsonl\"\n","\n","def get_current_progress(file_path):\n","    if os.path.exists(file_path):\n","        with jsonlines.open(file_path, mode='r') as reader:\n","            return sum(1 for _ in reader)\n","    return 0\n","\n","with jsonlines.open(f\"data/{dataset_id}.jsonl\", \"r\") as reader:\n","    samples = [sample for sample in reader.iter()]\n","\n","id_to_label = {\n","    0: 'positive',\n","    1: 'negative'\n","}\n","\n","backups = []\n","start_index = get_current_progress(annotations_file_path)\n","\n","with jsonlines.open(annotations_file_path, mode=\"a\") as writer:\n","    for i, sample in enumerate(samples[start_index:], start=start_index):\n","        print(f'{round(i / len(samples) * 100, 2)}% complete')\n","        print(f'Review: {sample[\"review\"]}')\n","        try:\n","            label = id_to_label[int(input('Sentiment: '))]\n","            writer.write({\"review\": sample[\"review\"], \"domain\": sample[\"domain\"], \"label\": label})\n","            backups.append((sample, label))\n","            clear_output(wait=False)\n","        except KeyboardInterrupt:\n","            break"],"metadata":{"id":"OA-4ve1qfjCE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uyP0GtHe_3vW"},"source":["## ðŸŽ¯ Q3.3: **Agreement Measure (12 pts)**\n","\n","TODOðŸ”»: Based on your and your partner's annotations in 3.2, calculate the [Cohen's Kappa](https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-kappa) or [Krippendorff's Alpha](https://github.com/pln-fing-udelar/fast-krippendorff) (if you are in a group of three students) between the annotators on **each domain** and **across all domains**.\n","\n","**Note:** Cohen's Kappa or Krippendorff's Alpha interpretation\n","\n","0: No Agreement\n","\n","0 ~ 0.2: Slight Agreement\n","\n","0.2 ~ 0.4: Fair Agreement\n","\n","0.4 ~ 0.6: Moderate Agreement\n","\n","0.6 ~ 0.8: Substantial Agreement\n","\n","0.8 ~ 1.0: Near Perfect Agreement\n","\n","1.0: Perfect Agreement\n","\n","**Questions:**\n","- What is the overall degree of agreement between you and your partner(s) according to the above interpretation of score ranges?\n","- In which domain are disagreements most and least frequently happen between you and your partner(s)? Give some examples to explain why that is the case.\n","- Are there possible ways to address the disagreements between annotators?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_2VlClO38Jt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712396474013,"user_tz":-120,"elapsed":335,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"4ed8a578-dcbd-421b-ce70-3b0bedd1c7f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cohen's Kappa: 0.5298372513562388\n","Krippendorff's Alpha: 0.5803066989507667\n"]}],"source":["# Fill your code for calculating agreement scores here.\n","from collections import Counter\n","\n","def compute_cohens_kappa(annotations1, annotations2):\n","\n","    total_annotations = len(annotations1)\n","    label_agreement = {}\n","    for label1, label2 in zip(annotations1, annotations2):\n","        if label1 == label2:\n","            if label1 in label_agreement:\n","                label_agreement[label1] += 1\n","            else:\n","                label_agreement[label1] = 1\n","\n","    observed_agreement = sum(label_agreement.values()) / total_annotations\n","\n","    annotator1_counts = Counter(annotations1)\n","    annotator2_counts = Counter(annotations2)\n","\n","    expected_agreement = sum((annotator1_counts[label] * annotator2_counts[label]) / (total_annotations ** 2)\n","                             for label in annotator1_counts)\n","\n","    kappa_score = (observed_agreement - expected_agreement) / (1 - expected_agreement)\n","\n","    return kappa_score\n","\n","def compute_alpha(annotations1, annotations2):\n","\n","    data = list(zip(annotations1, annotations2))\n","\n","    label_pairs = Counter(data)\n","\n","    observed_agreement = sum([label_pairs[label] for label in label_pairs if label[0] == label[1]]) / len(data)\n","\n","    total_pairs = sum(label_pairs.values())\n","    expected_agreement = sum([(label_pairs[label1] / total_pairs) * (label_pairs[label2] / total_pairs)\n","                              for label1 in label_pairs for label2 in label_pairs if label1[0] == label2[0]])\n","\n","    if expected_agreement == 1:\n","        return 1\n","    alpha_score = 1 - (1 - observed_agreement) / (1 - expected_agreement)\n","    return alpha_score\n","\n","def read_annotations(file_path):\n","    annotations = []\n","    with jsonlines.open(file_path, mode='r') as reader:\n","        for line in reader:\n","            annotations.append(line['label'])\n","    return annotations\n","\n","annotator1_file_path = \"data/3-329571.jsonl\"\n","annotator2_file_path = \"data/3-347729.jsonl\"\n","\n","annotator1_annotations = read_annotations(annotator1_file_path)\n","annotator2_annotations = read_annotations(annotator2_file_path)\n","\n","kappa_score = compute_cohens_kappa(annotator1_annotations, annotator2_annotations)\n","print(f\"Cohen's Kappa: {kappa_score}\")\n","\n","alpha_score = compute_alpha(annotator1_annotations, annotator2_annotations)\n","print(f\"Krippendorff's Alpha: {alpha_score}\")"]},{"cell_type":"code","source":["def read_annotations(file_path):\n","    annotations = []\n","    with jsonlines.open(file_path, mode='r') as reader:\n","        for line in reader:\n","            annotations.append({'review': line['review'], 'label': line['label'], 'domain': line['domain']})\n","    return annotations\n","\n","def print_domain_disagreements(domain, disagreements):\n","    print(f\"Domain: {domain}\")\n","    if disagreements:\n","        print(\"Examples of disagreements:\")\n","        for example in disagreements[:3]:\n","            print(\"Review : \", example[0]['review'])\n","            print(\"Annotation 1:\", example[0]['label'])\n","            print(\"Annotation 2:\", example[1]['label'])\n","            print(\"-\" * 50)\n","    else:\n","        print(\"No disagreements found.\")\n","\n","annotator1_annotations = read_annotations(annotator1_file_path)\n","annotator2_annotations = read_annotations(annotator2_file_path)\n","\n","domains = set(annotation['domain'] for annotation in annotator1_annotations)\n","\n","for domain in domains:\n","    domain_annotations1 = [annotation for annotation in annotator1_annotations if annotation['domain'] == domain]\n","    domain_annotations2 = [annotation for annotation in annotator2_annotations if annotation['domain'] == domain]\n","    disagreements = [(annot1, annot2) for annot1, annot2 in zip(domain_annotations1, domain_annotations2) if annot1['label'] != annot2['label']]\n","    print_domain_disagreements(domain, disagreements)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rI8u5o3ktmsl","executionInfo":{"status":"ok","timestamp":1712396606171,"user_tz":-120,"elapsed":355,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"0991d688-3d86-4257-e545-855ec983fcb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Domain: books\n","Examples of disagreements:\n","Review :  I used to wonder why Hollywood ignored the crimes of Soviet Communism, which easily rival the crimes of National Socialism, something they've shown us over and over and over again (certainly understandable). Not one major film has been made showing us the summary executions during Lenin's \"Red Terror,\" the deaths of millions during forced collectivization, the torture and mass murder during Stalin's purges of the late 30's, the cold-blooded massacre of thousands of Polish citizens at Katyn, or the horrors of the gulag. But Hollywood has been plenty busy pumping out films on \"McCarthyism\" and the ghastly \"blacklist.\" They're listed in this book: Guilty by Suspicion, The Front, The Way We Were, The Majestic, Marathon Man, The House on Carroll Street, Fellow Traveler, One of Hollywood Ten. This is all propaganda of course. The way they tell it you'd think it was one of the darkest and most tragic eras in human history. McCarthy and his ilk were no saints, and there were certainly excesses, but I don't believe anyone was murdered by McCarthy and HUAC. By contrast tens of millions were murdered in Soviet Russia. Apparently that's no big deal to the fools in Tinseltown who are constantly wringing their hands over \"McCarthyism,\" the blacklist and the persecuted Hollywood ten. These crybabies wouldn't have lasted ONE DAY in the Kolyma forced-labor camps! The Radoshes show us that Hollywood back in the 30's and 40's was filled with Soviet-sympathizing Communists who put out propaganda films whitewashing Stalin's mass murders such as The North Star, Song of Russia and Mission to Moscow. While the first two simply glossed over the Stalinist holocaust in Ukraine, the latter film actually GLORIFIED Stalin's secret police and mass purges. Can you believe this? Hollywood actually justified Stalin's killing machine! Think about that the next time some Hollywood leftist whines about the \"horrors\" of \"McCarthyism.\" While no one in Hollywood would dare praise Stalin now, quite a few do embrace other blood-soaked Communist despots. For example, Shirley MacLaine made a pilgrimage to Red China, then reeling in the aftermath of Chairman Mao's Cultural Revolution, and declared that Mao Tse-tung was a hero of all time. Someone should let Shirley know that her \"hero\" is the biggest mass murderer in history (see \"Mao: The Unknown Story\" by Jung Chang and Jon Halliday). And countless Hollywood \"useful idiots\" such as Robert Redford, Danny Glover, Oliver Stone and many, MANY others swoon over that thug Fidel Castro, who has more blood on his hands (15,000 killed) than the hated Augusto Pinochet (3,000 killed). Pinochet's crimes have been the subject of several Hollywood flicks - i.e. Missing, Of Love and Shadows, etc. None have been made on the mass murders of Castro and Mao. To add insult to injury Castroite Robert REDford was executive producer of \"The Motorcycle Diaries,\" a saintly portrayal of Castro's chief executioner Ernesto \"Che\" Guevara in his early days. He later went on to murder hundreds of \"counter-revolutionaries\" at La Cabana prison, something that's highly unlikely to be portrayed in the next pro-Che film starring Benicio del Toro. The negative reviews are beyond pathetic. McCarthy this... McCarthy that... blah, blah, blah... Of course they can't refute one thing in the book. They prefer myth, not facts. Hollywood was filled with Stalin-worshipping Reds (now Castro-worshipping Pinkos), the Rosenberg's were guilty as charged and Alger Hiss was a Soviet spy. Truth hurts - deal with it! Also recommended: Fidel: Hollywood's Favorite Tyrant by Humberto E. Fontova Hollywood Party: How Communism Seduced the American Film Industry in the 1930s and 1940s by Kenneth Lloyd Billingsley In Denial: Historians, Communism & Espionage by John Earl Haynes & Harvey Klehr The Venona Secrets: Exposing Soviet Espionage and America's Traitors by Herbert Romerstein and Eric Breindel\n","Annotation 1: negative\n","Annotation 2: positive\n","--------------------------------------------------\n","Domain: electronics\n","Examples of disagreements:\n","Review :  This once available product has served me well. Very dynamic, universal in use, and user friendly. What more could be asked of a simple battery backup system that doesn't cost hundreds of $. I'm looking for the replacement because it's best duty is under every critical user's computer\n","Annotation 1: positive\n","Annotation 2: negative\n","--------------------------------------------------\n","Review :  The walkman is easy to use with enough memory for my meager uses.  The sound is excellent, but I use an older set of Sony head phones rather then the ear pieces that came with the set. The unit recharges very quickly and again for my uses last a long time. It is quite small and the only disadvantage I've found is there is no clip or arrangement to carry it except a pocket\n","Annotation 1: positive\n","Annotation 2: negative\n","--------------------------------------------------\n","Review :  First off let me state for the record that I only buy Sony , but I have to advise you against the purchase of the bean walkman. I purchased one in March 2006 and returned it September 2006, as I had purchased replacement insurance and was able to return it because it broke. It is hard to turn off and the usb port cover always pops open, not to mention the headphones always pull out. The FM receiver is hard tune in the daylight as the screen is invisiable. Save your self the hassel!!!!!\n","Annotation 1: negative\n","Annotation 2: positive\n","--------------------------------------------------\n","Domain: dvd\n","Examples of disagreements:\n","Review :  Incredible does describe this movie- incredibly bad! This movie doesn't know what it wants to be. At times one can see attempts to recreate and milk the genuine rage and drama of \"Mississippi Burning\" or the anger and impunity of \"Ghosts of Mississippi\", or a hybrid of those and bits and pieces of other movies of this genre. At other times it looks like its just trying to draw from the sex appeal and cheesy interaction of 2 of its main actors, Matthew McConaughey and Sandra Bullock. As a result this movie picks up the rear in that genre's list of movies for lack of originality, and the scattered hodge podge statements it tries to make. Some of my favorite actors are wasted, either by being given cliched dialogue, over acting, or being put there merely for their brand name with no script to support their presence. If this script is considered a great adaptation of a great book, then I have no interest in reading the book. I'm sure the idea and the premise looked good on paper. But the direction doesn't guide the story with any coherence or identity of its own.  Bullock is mere cheesecake and is unbelievable in her role, proving she's only worthy of b-grade \"Miss Congeniality\" type crud.  McConaughey over acts so much (as can be said for Keifer Sutherland and others) that it can be said he balances out the under acting of Patrick McGoohan, Kevin Spacey and Donald Sutherland. Samuel L. Jackson should be ashamed of himself of not only his worst performance ever, but also easily his worst choice of movies to participate in.  Even the extras in this movie are horrible. Enough can't be said about the terrible direction in this movie, equating elongated facial shots with drama to a fault, making Sergio Leone westerns look much more authentic by comparison. This is not \"a powerful film.\" It's a powerful attempt at using a big name cast, a brand name book and a capable director to use material that they know would easily jerk the emotions of its audience, and in doing that they resort to all the cliches of this genre scattering the story line in so many directions, culminating in an over the top, pleading hunk of cheese summation scene by McConaughey that wouldn't hold water in any real court of justice. Maybe people claim greatness to this movie out of guilt, or because they haven't seen quality comparisons and classics like \"Mississippis Burning,\" that has both a great script and fabulous acting. I don't know. All I know is it tugs from so many hokey directions it takes the final scene to remind everyone how it started. Among the top of my most over rated movies\n","Annotation 1: negative\n","Annotation 2: positive\n","--------------------------------------------------\n","Domain: housewares\n","Examples of disagreements:\n","Review :  i got this for my cat (ocicat breed).  he's a short-haired cat that doesn't generally need much grooming except when the season changes.  shedender worked just like the commericals.  one brush and fur goes flying everywhere.  it does remove hair however the teeth of the comb are not deep so the removed hair goes either flying or stays on top of the fur towards the back.  as u can see in the picture the tool is small.  if you have the patience and extra time (and if your pet has the patience too!) u can think about saving some money with this tool.  if you want something more efficient, invest some extra money and go for the furminator instead.  i bought both and compared them but i'm keeping my furminator despite the much higher price\n","Annotation 1: negative\n","Annotation 2: positive\n","--------------------------------------------------\n","Review :  The way the brush is situated on the handle makes this brush difficult to use.  I would get the Furminator instead, it has the same deshedding edge but a much easier to use overall design and it comes in larger sizes!  This technology is great and you will be so shocked when you see all the hair you are able to remove\n","Annotation 1: negative\n","Annotation 2: positive\n","--------------------------------------------------\n","Review :  This product does what it claims.  I recieved it a few days ago and it has performed well.  The downside of the product is the size.  It does a good job if you have an hour or more (depending on the size of your pet) to spend using it.  They tout it's price in comparison to other models costing around forty dollars or more.  I wish I had invested in the bigger, more expensive FURminator.  I rate this product with two stars, only because it's a hassle and time consuming to use on a larger animal.  It would probably be worth it if you own a small dog or cat\n","Annotation 1: negative\n","Annotation 2: positive\n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":["def count_different_labels_by_domain(annotations1, annotations2, domains):\n","    different_label_counts_by_domain = {domain: 0 for domain in domains}\n","\n","    for annotation1, annotation2 in zip(annotations1, annotations2):\n","        domain = annotation1['domain']\n","        label1 = annotation1['label']\n","        label2 = annotation2['label']\n","        if label1 != label2:\n","            different_label_counts_by_domain[domain] += 1\n","\n","    return different_label_counts_by_domain\n","\n","annotator1_annotations = read_annotations(annotator1_file_path)\n","annotator2_annotations = read_annotations(annotator2_file_path)\n","\n","domains = set(annotation['domain'] for annotation in annotator1_annotations + annotator2_annotations)\n","\n","different_label_counts = count_different_labels_by_domain(annotator1_annotations, annotator2_annotations, domains)\n","\n","for domain, count in different_label_counts.items():\n","    print(f\"Domain: {domain}, Different Label Count: {count}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fa1kPD-9vi5a","executionInfo":{"status":"ok","timestamp":1712396485808,"user_tz":-120,"elapsed":439,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"d4fd0df4-be97-495c-a966-72639d3dd097"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Domain: books, Different Label Count: 1\n","Domain: electronics, Different Label Count: 4\n","Domain: dvd, Different Label Count: 1\n","Domain: housewares, Different Label Count: 7\n"]}]},{"cell_type":"markdown","metadata":{"id":"I98NGWYb3zl8"},"source":["Based on the interpretation of score ranges:\n","\n","1. **Overall Degree of Agreement:**\n","   - Cohen's Kappa: 0.5298\n","   - Krippendorff's Alpha: 0.5803\n","\n","   The degree of agreement between the annotators falls within moderate to substantial agreement according to Cohen's Kappa and Krippendorff's Alpha.\n","\n","2. **Disagreements Frequency by Domain:**\n","   - Most Frequent: \"housewares\" domain with 7 instances of disagreements.\n","   - Least Frequent: \"dvd\" and \"books\" domains, each with 1 instance of disagreement.\n","\n","   **Examples and Explanation:**\n","   - Most disagreements are in the \"Housewares\" domain because one reviewer (me) believe that even if the product is useful, it does not meet the expectations of the user. Indeed, they say the product does what it claims to do, however they suggest another product and praise the other product. For me it is a negative review since they would not recommend it.\n","   - **\"Housewares\" Domain:** Multiple disagreements in this domain due to varying preferences, experiences, or interpretations of product features. For instance, differences in usability assessments, as seen in the conflicting evaluations of product size and performance, contribute to disagreements.\n","   - **\"DVD\" and \"Books\" Domains:** Disagreements are less common in these domains because the criteria for evaluation is more objective. For example, in the \"books\" domain, the disagreement revolves around differing interpretations of historical events and political ideologies, which may be less subjective compared to product reviews.\n","\n","   In conclusion, to address the disagreement, we should agree on what a positive or negative review should be, on what criteria should be used for evaluation.\n","\n","3. **Addressing Disagreements:**\n","\n","   - Review 1 (Shedender Product):\n","      - Annotation 1 (Negative): The reviewer acknowledges that the Shedender removes fur but criticizes its design, mentioning that the teeth of the comb are not deep enough, causing removed hair to fly or stay on top of the fur. They suggest considering the Furminator instead for better efficiency.\n","      - Annotation 2 (Positive): In contrast, the second annotator praises the Shedender's performance, stating that it works as advertised and efficiently removes fur.\n","      - Explanation: The disagreement arises from differing perspectives on the product's effectiveness and usability. Annotator 1 focuses on specific design flaws and recommends an alternative, while Annotator 2 emphasizes the Shedender's functionality and cost-saving benefits.\n","\n","  - Review 2 (Brush Design):\n","      - Annotation 1 (Negative): The reviewer criticizes the brush's design, stating that it is difficult to use and recommends opting for the Furminator instead due to its easier usability and larger sizes.\n","      - Annotation 2 (Positive): Conversely, the second annotator praises the brush's deshedding capabilities and technological features, highlighting its effectiveness in removing hair.\n","      - Explanation: The disagreement revolves around the usability and effectiveness of the brush. Annotator 1 finds flaws in its design and suggests an alternative, while Annotator 2 appreciates its deshedding performance and technological aspects.\n","\n","  - Review 3 (Product Performance):\n","\n","    - Annotation 1 (Negative): The reviewer acknowledges the product's performance but criticizes its size and usability, mentioning that it is time-consuming to use, especially on larger animals.\n","    - Annotation 2 (Positive): In contrast, the second annotator praises the product's performance and effectiveness, stating that it performs well and efficiently removes pet hair.\n","    - Explanation: The disagreement arises also from differing opinions on the product's practicality and suitability for larger animals. Annotator 1 finds issues with its size and usability, while Annotator 2 focuses on its overall performance and effectiveness.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"t4wuRpHt-rQF"},"source":["<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n","\n","# Part 4: Data Augmentation (20 pts)\n","\n","Since we only used 20% of the whole dataset for training, which might limit the model performance. In the final part, we will try to enlarge the training set by **data augmentation**.  \n","\n","Specifically, we will **`Rephrase`** some current training samples using pretrained paraphraser. So that the paraphrased synthetic samples would preserve the semantic similarity while change the surface format.\n","\n","You can use the pretrained T5 paraphraser [here](https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base).\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"HQa2q1io_5Pk"},"source":["## ðŸŽ¯ Q4.1: **Data Augmentation with Paraphrasing (15 pts)**\n","TODOðŸ”»: Implement functions named `get_paraphrase_batch` and `get_paraphrase_dataset` with the details in the below two blocks."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"nTMdZ-azABk-","executionInfo":{"status":"ok","timestamp":1712402076247,"user_tz":-120,"elapsed":4782,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"56ce9964-755d-4cbc-d089-af24499b1ea2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# get the given pretrained paraphrase model and the corresponding tokenizer (https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base)\n","paraphrase_tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n","paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n","\n","def get_paraphrase_batch(\n","    model,\n","    tokenizer,\n","    input_samples,\n","    n,\n","    repetition_penalty=10.0,\n","    diversity_penalty=3.0,\n","    no_repeat_ngram_size=2,\n","    temperature=0.7,\n","    max_length=256,\n","    device='cuda:0'):\n","    '''\n","    Input\n","      model: paraphraser\n","      tokenizer: paraphrase tokenizer\n","      input_samples: a batch (list) of real samples to be paraphrased\n","      n: number of paraphrases to get for each input sample\n","      for other parameters, please refer to:\n","          https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig\n","    Output: Tuple.\n","      synthetic_samples: a list of paraphrased samples\n","    '''\n","\n","    # TODO: implement paraphrasing on a batch of imput samples\n","\n","    inputs = tokenizer(input_samples, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n","\n","    synthetic_samples = []\n","    for i in range(len(input_samples)):\n","        input_text = inputs.input_ids[i].unsqueeze(0)\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                input_text,\n","                max_length=max_length,\n","                num_return_sequences=n,\n","                repetition_penalty=repetition_penalty,\n","                diversity_penalty=diversity_penalty,\n","                no_repeat_ngram_size=no_repeat_ngram_size,\n","                temperature=temperature,\n","                # Changed from True to False because of this error\n","                # ValueError: `diversity_penalty` is not 0.0 or `num_beam_groups` is not 1, triggering group beam search. In this generation mode, `do_sample` must be set to `False`\n","                do_sample=False,\n","                top_k=50,\n","                top_p=0.95,\n","                eos_token_id=tokenizer.eos_token_id,\n","                pad_token_id=tokenizer.pad_token_id,\n","                early_stopping=True,\n","                num_beams=2,\n","                num_beam_groups=2\n","            )\n","        for output in outputs:\n","            paraphrase = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","            synthetic_samples.append(paraphrase)\n","\n","    return synthetic_samples"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"aL29QCAodF1b","executionInfo":{"status":"ok","timestamp":1712402076247,"user_tz":-120,"elapsed":3,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}}},"outputs":[],"source":["random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","data_dir = 'data'\n","data_train_path = os.path.join(data_dir, 'train_sa.jsonl')\n","BATCH_SIZE = 8\n","N_PARAPHRASE = 2\n","\n","def get_paraphrase_dataset(model, tokenizer, data_path, batch_size, n_paraphrase):\n","    '''\n","    Input\n","      model: paraphrase model\n","      tokenizer: paraphrase tokenizer\n","      data_path: path to the `jsonl` file of training data\n","      batch_size: number of input samples to be paraphrases in one batch\n","      n_paraphrase: number of paraphrased sequences for each sample\n","    Output:\n","      paraphrase_dataset: a list of all paraphrase samples. Do not include the original training data.\n","    '''\n","    paraphrase_dataset = []\n","    with jsonlines.open(data_path, \"r\") as reader:\n","\n","        samples = [sample for sample in reader]\n","\n","        for i in range(0, len(samples), batch_size):\n","            batch = samples[i:i + batch_size]\n","            input_samples = [sample[\"review\"] for sample in batch]\n","            domains_labels = [(sample[\"domain\"], sample[\"label\"]) for sample in batch]\n","\n","            synthetic_samples = get_paraphrase_batch(\n","                model,\n","                tokenizer,\n","                input_samples,\n","                n_paraphrase,\n","                repetition_penalty=10.0,\n","                diversity_penalty=3.0,\n","                no_repeat_ngram_size=2,\n","                temperature=0.7,\n","                max_length=256,\n","                device=device\n","            )\n","\n","            for j, (domain, label) in enumerate(domains_labels):\n","                for k in range(n_paraphrase):\n","                    index = j * n_paraphrase + k\n","                    paraphrased_review = synthetic_samples[index]\n","                    paraphrase_dataset.append({\"review\": paraphrased_review, \"domain\": domain, \"label\": label})\n","\n","    return paraphrase_dataset"]},{"cell_type":"markdown","metadata":{"id":"sH6Yr5i2fmaw"},"source":["**Note:** run paraphrasing, which will take ~20-30 minutes using a T4 Colab GPU. But the running time could depend on various implementations."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Xq-FvZoDfmax","colab":{"base_uri":"https://localhost:8080/"},"outputId":"846035d4-fdf7-4c37-a105-f7b070491137","executionInfo":{"status":"ok","timestamp":1712404506526,"user_tz":-120,"elapsed":322905,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}}},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]}],"source":["paraphrase_dataset = get_paraphrase_dataset(paraphrase_model, paraphrase_tokenizer, data_train_path, BATCH_SIZE, N_PARAPHRASE)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"nGQsvD4dktVv","executionInfo":{"status":"ok","timestamp":1712404506527,"user_tz":-120,"elapsed":0,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}}},"outputs":[],"source":["# Original training dataset\n","with jsonlines.open(data_train_path, \"r\") as reader:\n","    origin_data = [dt for dt in reader.iter()]\n","\n","all_data = origin_data + paraphrase_dataset\n","\n","# Write all the original and paraphrased data samples into training dataset\n","augmented_data_train_path = os.path.join(data_dir, 'augmented_train_sa.jsonl')\n","with jsonlines.open(augmented_data_train_path, \"w\") as writer:\n","    writer.write_all(all_data)\n","\n","assert len(all_data) == 3 * len(origin_data)"]},{"cell_type":"markdown","metadata":{"id":"PmFpfxrjWA1O"},"source":["## ðŸŽ¯ Q4.2: **Retrain RoBERTa Model with Data Augmentation (5 pts)**\n","TODOðŸ”»: Retrain the sentiment analysis model with the augmented (original+paraphrased), larger dataset :)\n","\n","**Note:** *Training on the augmented data will take about 15 minutes using a T4 Colab GPU.*"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"sz9gQSe8ANix","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712405124641,"user_tz":-120,"elapsed":283317,"user":{"displayName":"Malak Lahlou","userId":"06427181277494045962"}},"outputId":"392400b9-3b63-4af1-8732-0e647a32542d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Building SA Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["4800it [00:07, 663.98it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Building SA Dataset...\n"]},{"output_type":"stream","name":"stderr","text":["6400it [00:09, 692.44it/s] \n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training:   0%|          | 0/600 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n","Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:47<00:00, 12.73it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:17<00:00, 45.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0 | Training Loss: 0.663 | Validation Loss: 0.347\n","Epoch 0 SA Validation:\n","Confusion Matrix:\n","[[2957  243]\n"," [ 594 2606]]\n","F1: (87.60%, 86.16%) | Macro-F1: 86.88%\n","Model Saved!\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:46<00:00, 12.79it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:17<00:00, 46.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Training Loss: 0.398 | Validation Loss: 0.308\n","Epoch 1 SA Validation:\n","Confusion Matrix:\n","[[2918  282]\n"," [ 331 2869]]\n","F1: (90.49%, 90.35%) | Macro-F1: 90.42%\n","Model Saved!\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:46<00:00, 12.82it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:17<00:00, 45.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 2 | Training Loss: 0.285 | Validation Loss: 0.421\n","Epoch 2 SA Validation:\n","Confusion Matrix:\n","[[2809  391]\n"," [ 243 2957]]\n","F1: (89.86%, 90.32%) | Macro-F1: 90.09%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:46<00:00, 12.95it/s]\n","Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:17<00:00, 46.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 3 | Training Loss: 0.165 | Validation Loss: 0.453\n","Epoch 3 SA Validation:\n","Confusion Matrix:\n","[[2951  249]\n"," [ 349 2851]]\n","F1: (90.80%, 90.51%) | Macro-F1: 90.65%\n","Model Saved!\n"]}],"source":["from sa import SADataset\n","from sa import compute_metrics, train, evaluate\n","\n","# Re-train a RoBERTa SA model on the augmented training dataset\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_name = \"FacebookAI/roberta-base\"\n","tokenizer = RobertaTokenizer.from_pretrained(model_name)\n","model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","model.to(device)\n","\n","train_dataset = SADataset('data/augmented_train_sa.jsonl', tokenizer)\n","dev_dataset = SADataset('data/test_sa.jsonl', tokenizer)\n","\n","epochs = 4\n","max_grad_norm = 1.0\n","warmup_percent = 0.3\n","learning_rate = 1e-5\n","\n","train(train_dataset=train_dataset,\n","      dev_dataset=dev_dataset,\n","      model=model,\n","      device=device,\n","      batch_size=BATCH_SIZE,\n","      epochs=epochs,\n","      learning_rate=learning_rate,\n","      warmup_percent=warmup_percent,\n","      max_grad_norm=max_grad_norm,\n","      model_save_root='models/augmented/', tensorboard_path=\"./tensorboard/part4_lr{}\".format(learning_rate))"]},{"cell_type":"markdown","metadata":{"id":"1PRvIB7ZAyaE"},"source":["TODOðŸ”»: Discuss your results by answering the following questions\n","\n","- Compare the performances of models in Part 1 and Part 4. Does the data augmentation help with the performance and why (give possible reasons)?\n","- No matter whether the data augmentation helps or not, list **three** possible ways to improve our current data augmentation method."]},{"cell_type":"markdown","metadata":{"id":"b5-dVSsniH9N"},"source":["  * **Initial Performance**: The model trained with augmented data showed a notably better initial performance, indicating that the augmented data likely provides a **more varied set of examples**. This variety can help the model learn more general features early on, which are applicable across a broader spectrum of the data.\n","\n","  * **Generalization to Unseen Data**: The augmented data model exhibited higher or similar Macro-F1 scores across epochs compared to the model trained on normal data. Higher Macro-F1 scores suggest **better generalization** to unseen data, possibly because the augmented data introduces the model to a wider range of linguistic variations and sentiment expressions, reducing the model's overfitting on the training set's specific characteristics.\n","\n","  * **Effectiveness in Learning**: The performance improvements with augmented data indicate that augmentation can effectively enhance the model's learning, as seen by the relatively quick achievement of high F1 scores. This suggests that data augmentation can mitigate some of the challenges associated with limited or unbalanced training data. This augmentation can help the model by providing more examples of underrepresented classes **(more balanced data)**.\n","\n","\n","### Three Ways to Improve Data Augmentation\n","\n","1. **Contextual Augmentation**: Add new examples to the training dataset that demonstrate new contexts, new domains for the model to learn from.\n"," Instead of text generation, use more complex techniques to create new sentences that maintain the original sentiment while altering the sentence structure. Techniques like paraphrasing or employing models that understand context can generate more meaningful variations.\n","\n","2. **Adversarial Filtering Algorithms**: To refine the dataset by removing examples that are too simplistic or might encourage shortcut learning, where the model relies on superficial cues rather than understanding the text. Through adversarial filtering, complex examples are prioritized in the training set, encouraging the model to develop more robust learning strategies. This involves identifying and excluding \"easy\" examples that don't contribute to learning complex patterns.\n","\n","\n","3. **Controlled Augmentation**: Construct controlled datasets that test specific dimensions of what we want in the first place By implementing a strategy to ensure that the augmented data does not disproportionately represent certain domains or sentiments. An example to control it is to monitor the distribution of key features in the augmented dataset and adjusting the augmentation process to avoid biases.\n"]},{"cell_type":"markdown","metadata":{"id":"M3CIeN_kaOQl"},"source":["<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n","\n","### **5 Upload Your Notebook, Data and Models**\n","\n","Please upload your filled jupyter notebook in your GitHub Classroom repository, **with all cells run and output results shown**.\n","\n","**Note:** We are **not** responsible for re-running the cells in your notebook.\n","\n","Please also submit all your **datasets** **(anotated and augmented)**, as well as **all your trained models** in Part 1 and Part 4, in your GitHub Classroom repository.\n","    \n","</div>"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"26ca3a98423d41a4bbce05d41bd42702":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_34a4a962d6204853b763cfc5d301c85c","IPY_MODEL_d222118099144ead9cc38e2f4219e275","IPY_MODEL_30495395af374e1bb0a6b15930a63ba2"],"layout":"IPY_MODEL_8ddb20dbe27a408c930846d8d829fced"}},"34a4a962d6204853b763cfc5d301c85c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8455e18213ab46689ed279898e63e977","placeholder":"â€‹","style":"IPY_MODEL_d75134905e8448e68b992bd695e8e214","value":"tokenizer_config.json:â€‡100%"}},"d222118099144ead9cc38e2f4219e275":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b5ebefb2b1e4dad8a0e33d057998886","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_246b33abb3524ee28a91044ba34e1819","value":25}},"30495395af374e1bb0a6b15930a63ba2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ae037058b4e480d8173b7a567296aed","placeholder":"â€‹","style":"IPY_MODEL_d0f4d2b2c4464198bd6223deb9fa13e4","value":"â€‡25.0/25.0â€‡[00:00&lt;00:00,â€‡1.61kB/s]"}},"8ddb20dbe27a408c930846d8d829fced":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8455e18213ab46689ed279898e63e977":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d75134905e8448e68b992bd695e8e214":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b5ebefb2b1e4dad8a0e33d057998886":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"246b33abb3524ee28a91044ba34e1819":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3ae037058b4e480d8173b7a567296aed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0f4d2b2c4464198bd6223deb9fa13e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d23aed683334472485b7d11704428885":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef7f23a0cd5f45bebeb678152f0d60a6","IPY_MODEL_39f8398b3b56470b85316e96f329e1e3","IPY_MODEL_4eb54466a61b42c4b20490ccbb42b406"],"layout":"IPY_MODEL_7cdf0b6e820e44998b394481cae9267b"}},"ef7f23a0cd5f45bebeb678152f0d60a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d90ae1031ca847359418bf24d505a2ea","placeholder":"â€‹","style":"IPY_MODEL_4c041f02fe2b4f28a011a6288700d931","value":"vocab.json:â€‡100%"}},"39f8398b3b56470b85316e96f329e1e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_96ccb3fd3df74e2284ec48d02ac5cc83","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c73d4b041cac4c909737eb8b5b34f162","value":898823}},"4eb54466a61b42c4b20490ccbb42b406":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb291126b4974732af114751db8e70f6","placeholder":"â€‹","style":"IPY_MODEL_79538cf1d0694db2a386f76b58c7e514","value":"â€‡899k/899kâ€‡[00:00&lt;00:00,â€‡1.13MB/s]"}},"7cdf0b6e820e44998b394481cae9267b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d90ae1031ca847359418bf24d505a2ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c041f02fe2b4f28a011a6288700d931":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96ccb3fd3df74e2284ec48d02ac5cc83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c73d4b041cac4c909737eb8b5b34f162":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eb291126b4974732af114751db8e70f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79538cf1d0694db2a386f76b58c7e514":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48de2470b898473aa82ea4fd4fb4e167":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1dfdf36317f542aab0d1cbba07c61390","IPY_MODEL_a97de606ffa94efda1aa087c37a7884b","IPY_MODEL_d9205800b28d4d81bffc319fe0c6d482"],"layout":"IPY_MODEL_a726addf54a14ce48fb5fc0d75bc672b"}},"1dfdf36317f542aab0d1cbba07c61390":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d48d8de98db422f8e882b7801057875","placeholder":"â€‹","style":"IPY_MODEL_b39358b196e141a7847641bd8043fc6d","value":"merges.txt:â€‡100%"}},"a97de606ffa94efda1aa087c37a7884b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2546b8f6abc0471a9fbce9be7efcac47","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5219176ea7bf4684be6a034cf1e6cda6","value":456318}},"d9205800b28d4d81bffc319fe0c6d482":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba6a13612af24266a1e97371810594ce","placeholder":"â€‹","style":"IPY_MODEL_f09c7589deeb4e75ace35a91ebb02f7f","value":"â€‡456k/456kâ€‡[00:00&lt;00:00,â€‡767kB/s]"}},"a726addf54a14ce48fb5fc0d75bc672b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d48d8de98db422f8e882b7801057875":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b39358b196e141a7847641bd8043fc6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2546b8f6abc0471a9fbce9be7efcac47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5219176ea7bf4684be6a034cf1e6cda6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba6a13612af24266a1e97371810594ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f09c7589deeb4e75ace35a91ebb02f7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c37f5cd2c384650ae04ad6a221f24e5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_218825ebc9eb47eeb686cc66f905229d","IPY_MODEL_041a5c9907bb4cb3883cf5cea5f821e9","IPY_MODEL_d9a930cf63e748bc927ddc333c0b280c"],"layout":"IPY_MODEL_64e06f46aeb04472b5696ed6e7ba8268"}},"218825ebc9eb47eeb686cc66f905229d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_786e3889321c43b4862e774dfb837e63","placeholder":"â€‹","style":"IPY_MODEL_5811ae4956ed4e2aa7798d02807b5a65","value":"tokenizer.json:â€‡100%"}},"041a5c9907bb4cb3883cf5cea5f821e9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f128535cb1614c0fb12e386947ee826d","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eaf8593454ff47f18aa298ddbff6b41d","value":1355863}},"d9a930cf63e748bc927ddc333c0b280c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dd960565d4b40e8b59b1a5d418e9a1c","placeholder":"â€‹","style":"IPY_MODEL_17d974f8644f411da416f3c6a0a6581e","value":"â€‡1.36M/1.36Mâ€‡[00:00&lt;00:00,â€‡1.70MB/s]"}},"64e06f46aeb04472b5696ed6e7ba8268":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"786e3889321c43b4862e774dfb837e63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5811ae4956ed4e2aa7798d02807b5a65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f128535cb1614c0fb12e386947ee826d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eaf8593454ff47f18aa298ddbff6b41d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5dd960565d4b40e8b59b1a5d418e9a1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17d974f8644f411da416f3c6a0a6581e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca5d77cf8fb048caad685d591d1d4590":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c6abfef27b5b4f52972045d9cefcaceb","IPY_MODEL_6d654b3209094a1e9c3343243eaa2022","IPY_MODEL_7e58a3c447604e688804f4521a88343d"],"layout":"IPY_MODEL_0ab5b39f4883459e913ff5a585f541c7"}},"c6abfef27b5b4f52972045d9cefcaceb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ce0adf80b86495aa317189faa734347","placeholder":"â€‹","style":"IPY_MODEL_d6336026bba24b4b87668e131b836e45","value":"config.json:â€‡100%"}},"6d654b3209094a1e9c3343243eaa2022":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_21d3b619fe9d48c7b34f38db67e6e24d","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_afdd51d6ed634f959bad51ee7a1a225a","value":481}},"7e58a3c447604e688804f4521a88343d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_571e69ac328046e0a84a6f5ac99f0711","placeholder":"â€‹","style":"IPY_MODEL_fa8a0666b9e244be9e03d052bd32fbbf","value":"â€‡481/481â€‡[00:00&lt;00:00,â€‡30.6kB/s]"}},"0ab5b39f4883459e913ff5a585f541c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ce0adf80b86495aa317189faa734347":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6336026bba24b4b87668e131b836e45":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21d3b619fe9d48c7b34f38db67e6e24d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afdd51d6ed634f959bad51ee7a1a225a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"571e69ac328046e0a84a6f5ac99f0711":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa8a0666b9e244be9e03d052bd32fbbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa45b9b8555f44f5834a67966b43f317":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb568c468b194009b42a03334214378c","IPY_MODEL_c986f050552a47dc9e45e218796c73ab","IPY_MODEL_d144f362705149648bbcb79b88ab3269"],"layout":"IPY_MODEL_9861b0b1732b48418d3128f41be248bb"}},"eb568c468b194009b42a03334214378c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00f51ab450b449d2906e2901c05c6d9a","placeholder":"â€‹","style":"IPY_MODEL_a18e182295944db79419ae71963a69ae","value":"model.safetensors:â€‡100%"}},"c986f050552a47dc9e45e218796c73ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_755415e3599446e2825770d50eac34b3","max":498818054,"min":0,"orientation":"horizontal","style":"IPY_MODEL_391f62f003994c6da88a89abf4a51f83","value":498818054}},"d144f362705149648bbcb79b88ab3269":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f1c019906a9493783b176accb3a733b","placeholder":"â€‹","style":"IPY_MODEL_b8bca4aa51c3493886a0d5dbb4ee858b","value":"â€‡499M/499Mâ€‡[00:06&lt;00:00,â€‡77.8MB/s]"}},"9861b0b1732b48418d3128f41be248bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00f51ab450b449d2906e2901c05c6d9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a18e182295944db79419ae71963a69ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"755415e3599446e2825770d50eac34b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"391f62f003994c6da88a89abf4a51f83":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8f1c019906a9493783b176accb3a733b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8bca4aa51c3493886a0d5dbb4ee858b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}